{"lowest": [{"prediction": 0.04657374322414398, "code": "    /**\n     * Call getAttribute on an element, producing a meaningful error message if not present, or\n     * empty. It is an error for 'element' or 'attribute' to be null.\n     *\n     * @param element   The element to get the attribute value of\n     * @param attribute The name of the attribute to get\n     */\n    protected String requireAttribute(IXMLElement element, String attribute)\n            throws CompilerException {\n        String value = element.getAttribute(attribute);\n        if (value == null)\n        {\n            parseError(element, \"<\" + element.getName() + \"> requires attribute '\" + attribute\n                    + \"'\");\n        }\n        return value;\n    }", "nl_input": "Considering conditions in resources\nHello \n\nI want to built an installer for two different brands. Therefore I need different images on the panels, i.e \"Heading.image\" or \"Installer.image\" \nI managed to do this on the panels by registering a GUIListener at installerFrame and adjusting the images when switching panel. \nBut now I have two problems: \na) The explained way is not really nice \nb) This is the point: I cannot use different images on LanguageDialog! \n\nWouldn't it be nice to have the possibility of using conditions for resources? This would solve both of my problems. \n\ni.e.: \n... \n\n\n\n \n... \n\nThank you in advance for your answer.", "code_len": 153, "nl_len": 166, "path": "src/lib/com/izforge/izpack/compiler/CompilerConfig.java"}, {"prediction": 0.04726016893982887, "code": "    /**\n     * Get a required attribute of an element, ensuring it is an integer. A meaningful error message\n     * is generated as a CompilerException if not present or parseable as an int. It is an error for\n     * 'element' or 'attribute' to be null.\n     *\n     * @param element   The element to get the attribute value of\n     * @param attribute The name of the attribute to get\n     */\n    protected int requireIntAttribute(IXMLElement element, String attribute)\n            throws CompilerException {\n        String value = element.getAttribute(attribute);\n        if (value == null || value.length() == 0)\n        {\n            parseError(element, \"<\" + element.getName() + \"> requires attribute '\" + attribute\n                    + \"'\");\n        }\n        try\n        {\n            return Integer.parseInt(value);\n        }\n        catch (NumberFormatException x)\n        {\n            parseError(element, \"'\" + attribute + \"' must be an integer\");\n        }\n        return 0; // never happens\n    }", "nl_input": "Considering conditions in resources\nHello \n\nI want to built an installer for two different brands. Therefore I need different images on the panels, i.e \"Heading.image\" or \"Installer.image\" \nI managed to do this on the panels by registering a GUIListener at installerFrame and adjusting the images when switching panel. \nBut now I have two problems: \na) The explained way is not really nice \nb) This is the point: I cannot use different images on LanguageDialog! \n\nWouldn't it be nice to have the possibility of using conditions for resources? This would solve both of my problems. \n\ni.e.: \n... \n\n\n\n \n... \n\nThank you in advance for your answer.", "code_len": 231, "nl_len": 166, "path": "src/lib/com/izforge/izpack/compiler/CompilerConfig.java"}, {"prediction": 0.04917753487825394, "code": "    /**\n     * Add a name value pair to the project property set. It is <i>not</i> replaced it is already\n     * in the set of properties.\n     *\n     * @param name  the name of the property\n     * @param value the value to set\n     *\n     * @return true if the property was not already set\n     */\n    public boolean addProperty(String name, String value) {\n        return compiler.addProperty(name, value);\n    }", "nl_input": "Considering conditions in resources\nHello \n\nI want to built an installer for two different brands. Therefore I need different images on the panels, i.e \"Heading.image\" or \"Installer.image\" \nI managed to do this on the panels by registering a GUIListener at installerFrame and adjusting the images when switching panel. \nBut now I have two problems: \na) The explained way is not really nice \nb) This is the point: I cannot use different images on LanguageDialog! \n\nWouldn't it be nice to have the possibility of using conditions for resources? This would solve both of my problems. \n\ni.e.: \n... \n\n\n\n \n... \n\nThank you in advance for your answer.", "code_len": 110, "nl_len": 166, "path": "src/lib/com/izforge/izpack/compiler/CompilerConfig.java"}, {"prediction": 0.05103688687086105, "code": "    /**\n     * Gets the type.\n     * @return the type\n     */\n    public String getType() {\n        return _type;\n    }", "nl_input": "tweak JSON serialization to recognize javabean setters with return values\nOur default serialization behavior recognizes this as a setter:\n{code}\npublic void setAmount(double amount) {\n    this.amount = amount;\n}\n{code}\n\nBut not this:\n{code}\npublic Offer setAmount(double amount) {\n    this.amount = amount;\n    return this;\n}\n{code}\n\nEnd result is that deserialization doesn't set the field and the user is left scratching their head and demanding justice.  Look into whether we can tweak the default behavior to recognize the latter example as a setter in serialization.", "code_len": 35, "nl_len": 142, "path": "serial/base/src/main/java/org/switchyard/serial/graph/node/AccessNode.java"}, {"prediction": 0.0516323558986187, "code": "    /**\n     * Returns the value of the named attribute.\n     *\n     * @param attr the name of the attribute\n     * @return the value of the attribute or null if not set\n     * @see #setAttribute\n     */\n    public Object getAttribute(String attr)\n    {\n        return attributes.get(attr);\n    }", "nl_input": "Add setting a variable $INSTALL_DRIVE with the target drive letter for Windows systems\nI have the requirement to get the driver letter part of a Windows installation path for customization purposes. I added a pre-tested and documented patch for the current IzPack trunk.", "code_len": 76, "nl_len": 55, "path": "src/lib/com/izforge/izpack/installer/AutomatedInstallData.java"}], "q1": [{"prediction": 0.20762234926223755, "code": " /**\n  * Create a protocol buffer GetStoreFileRequest for a given region name\n  *\n  * @param regionName the name of the region to get info\n  * @param family the family to get store file list\n  * @return a protocol buffer GetStoreFileRequest\n  */\n public static GetStoreFileRequest\n     buildGetStoreFileRequest(final byte[] regionName, final byte[] family) {\n   GetStoreFileRequest.Builder builder = GetStoreFileRequest.newBuilder();\n   RegionSpecifier region = buildRegionSpecifier(\n     RegionSpecifierType.REGION_NAME, regionName);\n   builder.setRegion(region);\n   builder.addFamily(ByteString.copyFrom(family));\n   return builder.build();\n }", "nl_input": "Adding some fuction to check if a table/region is in compaction\nThis feature will be helpful to find out if a major compaction is going on.\nWe can show if it is in any minor compaction too.", "code_len": 162, "nl_len": 44, "path": "src/main/java/org/apache/hadoop/hbase/protobuf/RequestConverter.java"}, {"prediction": 0.20762622356414795, "code": "  private static void startCompactorWorkers(HiveConf conf) throws Exception {\n    int numWorkers = HiveConf.getIntVar(conf, HiveConf.ConfVars.HIVE_COMPACTOR_WORKER_THREADS);\n    for (int i = 0; i < numWorkers; i++) {\n      MetaStoreThread worker =\n          instantiateThread(\"org.apache.hadoop.hive.ql.txn.compactor.Worker\");\n      initializeAndStartThread(worker, conf);\n    }\n  }", "nl_input": "A method to extrapolate columnStats for partitions of a table\nIn a PARTITIONED table, there are many partitions. For example, \n\ncreate table if not exists loc_orc (\n  state string,\n  locid int,\n  zip bigint\n) partitioned by(year string) stored as orc;\n\nWe assume there are 4 partitions, partition(year='2000'), partition(year='2001'), partition(year='2002') and partition(year='2003').\n\nWe can use the following command to compute statistics for columns state,locid of partition(year='2001')\n\nanalyze table loc_orc partition(year='2001') compute statistics for columns state,locid;\n\nWe need to know the \u201caggregated\u201d column status for the whole table loc_orc. However, we may not have the column status for some partitions, e.g., partition(year='2002') and also we may not have the column status for some columns, e.g., zip bigint for partition(year='2001')\n\nWe propose a method to extrapolate the missing column status for the partitions.\n", "code_len": 112, "nl_len": 243, "path": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java"}, {"prediction": 0.20763757824897766, "code": "  boolean unprotectedSetTimes(String src, long mtime, long atime, boolean force) \n    throws UnresolvedLinkException {\n    writeLock();\n    try {\n      INodeFile inode = getFileINode(src);\n      return unprotectedSetTimes(src, inode, mtime, atime, force);\n    } finally {\n      writeUnlock();\n    }\n  }", "nl_input": "Intrinsic limits for HDFS files, directories\nEnforce a configurable limit on:\n  the length of a path component\n  the number of names in a directory\n\nThe intention is to prevent a too-long name or a too-full directory. This is not about RPC buffers, the length of command lines, etc. There may be good reasons for those kinds of limits, but that is not the intended scope of this feature. Consequently, a reasonable implementation might be to extend the existing quota checker so that it faults the creation of a name that violates the limits. This strategy of faulting new creation evades the problem of existing names or directories that violate the limits.", "code_len": 84, "nl_len": 143, "path": "src/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java"}, {"prediction": 0.20764467120170593, "code": "    public boolean equals(cancel_delegation_token_result that) {\n      if (that == null)\n        return false;\n\n      boolean this_present_o1 = true && this.isSetO1();\n      boolean that_present_o1 = true && that.isSetO1();\n      if (this_present_o1 || that_present_o1) {\n        if (!(this_present_o1 && that_present_o1))\n          return false;\n        if (!this.o1.equals(that.o1))\n          return false;\n      }\n\n      return true;\n    }", "nl_input": "Passing user identity from metastore client to server in non-secure mode\nCurrently in unsecure mode client don't pass on user identity. As a result hdfs and other operations done by server gets executed by user running metastore process instead of being done in context of client. This results in problem as reported here: \nhttp://mail-archives.apache.org/mod_mbox/hive-user/201111.mbox/%3CCAK0mCrRC3aPqtRHDe2J25Rm0JX6TS1KXxd7KPjqJjoqBjg=APA@mail.gmail.com%3E", "code_len": 141, "nl_len": 137, "path": "metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java"}, {"prediction": 0.20766212046146393, "code": "    public void get_index_by_name(String db_name, String tbl_name, String index_name, org.apache.thrift.async.AsyncMethodCallback<get_index_by_name_call> resultHandler) throws org.apache.thrift.TException {\n      checkReady();\n      get_index_by_name_call method_call = new get_index_by_name_call(db_name, tbl_name, index_name, resultHandler, this, ___protocolFactory, ___transport);\n      this.___currentMethod = method_call;\n      ___manager.call(method_call);\n    }", "nl_input": "Passing user identity from metastore client to server in non-secure mode\nCurrently in unsecure mode client don't pass on user identity. As a result hdfs and other operations done by server gets executed by user running metastore process instead of being done in context of client. This results in problem as reported here: \nhttp://mail-archives.apache.org/mod_mbox/hive-user/201111.mbox/%3CCAK0mCrRC3aPqtRHDe2J25Rm0JX6TS1KXxd7KPjqJjoqBjg=APA@mail.gmail.com%3E", "code_len": 142, "nl_len": 137, "path": "metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java"}], "median": [{"prediction": 0.2981756925582886, "code": "  /**\n   * Factory method to get StrExprProcessor.\n   *\n   * @return StrExprProcessor.\n   */\n  public StrExprProcessor getStrExprProcessor() {\n    return new StrExprProcessor();\n  }", "nl_input": "Parse wide OR and wide AND trees to flat OR/AND trees\nDeep trees of AND/OR are hard to traverse particularly when they are merely the same structure in nested form as a version of the operator that takes an arbitrary number of args.\n\nOne potential way to convert the DFS searches into a simpler BFS search is to introduce a new Operator pair named ALL and ANY.\n\nALL(A, B, C, D, E) represents AND(AND(AND(AND(E, D), C), B), A)\n\nANY(A, B, C, D, E) represents OR(OR(OR(OR(E, D), C),B),A)\n\nThe SemanticAnalyser would be responsible for generating these operators and this would mean that the depth and complexity of traversals for the simplest case of wide AND/OR trees would be trivial.", "code_len": 51, "nl_len": 187, "path": "ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java"}, {"prediction": 0.29820147156715393, "code": "    public void endFunction(String function, MetaStoreEndFunctionContext context) {\n      try {\n        Metrics.endScope(function);\n      } catch (IOException e) {\n        LOG.debug(\"Exception when closing metrics scope\" + e);\n      }\n\n      for (MetaStoreEndFunctionListener listener : endFunctionListeners) {\n        listener.onEndFunction(function, context);\n      }\n    }", "nl_input": "Enable QOP configuration for Hive Server 2 thrift transport\nThe QoP for hive server 2 should be configurable to enable encryption. A new configuration should be exposed \"hive.server2.thrift.sasl.qop\". This would give greater control configuring hive server 2 service.", "code_len": 91, "nl_len": 57, "path": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java"}, {"prediction": 0.29821664094924927, "code": "    /**\n     * Loads custom data like listener and lib references if exist and fills the installdata.\n     *\n     * @param installdata        installdata into which the custom action data should be stored\n     * @param bindeableContainer\n     * @throws Exception\n     */\n    protected void loadCustomData(AutomatedInstallData installdata, BindeableContainer bindeableContainer, PathResolver pathResolver) throws IOException, InstallerException, ClassNotFoundException {\n        IzpackProjectInstaller izpackModel = (IzpackProjectInstaller) readObject(\"izpackInstallModel\");\n        List<InstallerListener> customActions = new ArrayList<InstallerListener>();\n        for (Listener listener : izpackModel.getListeners()) {\n            if (!OsConstraintHelper.oneMatchesCurrentSystem(listener.getOs())) {\n                continue;\n            }\n            switch (listener.getStage()) {\n                case install:\n                    Class aClass = classPathCrawler.searchClassInClassPath(listener.getClassname());\n                    bindeableContainer.addComponent(aClass);\n                    customActions.add((InstallerListener) bindeableContainer.getComponent(aClass));\n                    break;\n                case uninstall:\n            }\n        }\n        installdata.setInstallerListener(customActions);\n        // uninstallerLib list if exist\n\n    }", "nl_input": "Support the creation of a temporary directory at install time which is cleaned up when install completes\nA temporary directory which can be referenced in the install.xml would make the management of files needed at install time much simpler. The attached patch (to the git head as of 2010/08/24) adds support for one or more temporary directories which are automatically cleaned up when the installer completes. To aid in debugging temporary directories are not deleted when tracing is enabled.", "code_len": 289, "nl_len": 91, "path": "izpack-installer/src/main/java/com/izforge/izpack/installer/container/provider/AbstractInstallDataProvider.java"}, {"prediction": 0.2982175946235657, "code": "    public static org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos.WarmupRegionResponse getDefaultInstance() {\n      return DEFAULT_INSTANCE;\n    }", "nl_input": "Shell tool to clear compaction queues\nscenario\uff1a\n1. Compact a table by mistake\n2. Compact is not completed within the specified time period\n\nIn this case, clearing the queue is a better choice, so as not to affect the stability of the cluster", "code_len": 40, "nl_len": 55, "path": "hbase-protocol-shaded/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/generated/AdminProtos.java"}, {"prediction": 0.2982325851917267, "code": "    public List<String> recv_get_partition_names() throws MetaException, org.apache.thrift.TException\n    {\n      get_partition_names_result result = new get_partition_names_result();\n      receiveBase(result, \"get_partition_names\");\n      if (result.isSetSuccess()) {\n        return result.success;\n      }\n      if (result.o2 != null) {\n        throw result.o2;\n      }\n      throw new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.MISSING_RESULT, \"get_partition_names failed: unknown result\");\n    }", "nl_input": "Passing user identity from metastore client to server in non-secure mode\nCurrently in unsecure mode client don't pass on user identity. As a result hdfs and other operations done by server gets executed by user running metastore process instead of being done in context of client. This results in problem as reported here: \nhttp://mail-archives.apache.org/mod_mbox/hive-user/201111.mbox/%3CCAK0mCrRC3aPqtRHDe2J25Rm0JX6TS1KXxd7KPjqJjoqBjg=APA@mail.gmail.com%3E", "code_len": 148, "nl_len": 137, "path": "metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java"}], "q3": [{"prediction": 0.46048644185066223, "code": "      public com.google.protobuf.Descriptors.Descriptor\n          getDescriptorForType() {\n        return org.apache.hadoop.hbase.protobuf.generated.FilterProtos.SkipFilter.getDescriptor();\n      }", "nl_input": "Implement fast-forwarding FuzzyRowFilter to allow filtering rows e.g. by \"???alex?b\"\nImplement fuzzy row key filter to allow fetching records e.g. by this criteria: \"???alex?b\".\n\nThis seems to be very useful as an alternative to select records by row keys by specifying their part which is not prefix part. Due to fast-forwarding nature of the filter in many situations this helps to avoid heavy full-table scans.\n\nThis is especially effective when you have composite row key and (some of) its parts has fixed length. E.g. with the key of format userId_actionId_time, given that userId and actionId length is fixed, one can select user actions of specific type using fuzzy row key by specifying mask \"????_myaction\". Given fast-forwarding nature of filter, this will usually work much faster than doing whole table scan with any of the existing server-side filters.\n\nIn many cases this can work as secondary-indexing alternative.\n\nMany times users implement it as a custom filter and many times they just don' know this is possible. Let's add it to the common codebase.", "code_len": 46, "nl_len": 246, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/generated/FilterProtos.java"}, {"prediction": 0.4605123996734619, "code": "    @Override\n    public void visit(XMLSerialize obj) {\n    \tmarkInvalid(obj, \"Pushdown of XMLSerialize not allowed\"); //$NON-NLS-1$\n    }", "nl_input": "Add support for not null and auto increment for temp table columns\nIt would be good to have auto increment and not null support for temp tables.", "code_len": 41, "nl_len": 29, "path": "engine/src/main/java/org/teiid/query/optimizer/relational/rules/CriteriaCapabilityValidatorVisitor.java"}, {"prediction": 0.46054884791374207, "code": "\t@Override\n\tpublic TaskManagerRuntimeInfo getTaskManagerInfo() {\n\t\treturn taskManagerInfo;\n\t}", "nl_input": "Add Rescalable Non-Partitioned State\nThis issue is associated with [FLIP-8| https://cwiki.apache.org/confluence/display/FLINK/FLIP-8%3A+Rescalable+Non-Partitioned+State].", "code_len": 27, "nl_len": 54, "path": "flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/RuntimeEnvironment.java"}, {"prediction": 0.4605530798435211, "code": "    public static org.apache.hadoop.hbase.protobuf.generated.FilterProtos.SkipFilter parseFrom(byte[] data)\n        throws com.google.protobuf.InvalidProtocolBufferException {\n      return newBuilder().mergeFrom(data).buildParsed();\n    }", "nl_input": "Implement fast-forwarding FuzzyRowFilter to allow filtering rows e.g. by \"???alex?b\"\nImplement fuzzy row key filter to allow fetching records e.g. by this criteria: \"???alex?b\".\n\nThis seems to be very useful as an alternative to select records by row keys by specifying their part which is not prefix part. Due to fast-forwarding nature of the filter in many situations this helps to avoid heavy full-table scans.\n\nThis is especially effective when you have composite row key and (some of) its parts has fixed length. E.g. with the key of format userId_actionId_time, given that userId and actionId length is fixed, one can select user actions of specific type using fuzzy row key by specifying mask \"????_myaction\". Given fast-forwarding nature of filter, this will usually work much faster than doing whole table scan with any of the existing server-side filters.\n\nIn many cases this can work as secondary-indexing alternative.\n\nMany times users implement it as a custom filter and many times they just don' know this is possible. Let's add it to the common codebase.", "code_len": 59, "nl_len": 246, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/generated/FilterProtos.java"}, {"prediction": 0.46055805683135986, "code": "    public boolean hasValue() {\n      return ((bitField0_ & 0x00000002) == 0x00000002);\n    }", "nl_input": "Implement fast-forwarding FuzzyRowFilter to allow filtering rows e.g. by \"???alex?b\"\nImplement fuzzy row key filter to allow fetching records e.g. by this criteria: \"???alex?b\".\n\nThis seems to be very useful as an alternative to select records by row keys by specifying their part which is not prefix part. Due to fast-forwarding nature of the filter in many situations this helps to avoid heavy full-table scans.\n\nThis is especially effective when you have composite row key and (some of) its parts has fixed length. E.g. with the key of format userId_actionId_time, given that userId and actionId length is fixed, one can select user actions of specific type using fuzzy row key by specifying mask \"????_myaction\". Given fast-forwarding nature of filter, this will usually work much faster than doing whole table scan with any of the existing server-side filters.\n\nIn many cases this can work as secondary-indexing alternative.\n\nMany times users implement it as a custom filter and many times they just don' know this is possible. Let's add it to the common codebase.", "code_len": 28, "nl_len": 246, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/generated/HBaseProtos.java"}], "highest": [{"prediction": 0.937376856803894, "code": "    /** \n     * @see org.teiid.adminapi.MonitoringAdmin#getQueueWorkerPools(java.lang.String)\n     * @since 4.3\n     */\n    public Collection getQueueWorkerPools(String identifier) \n        throws AdminException {\n        \n        if (identifier == null || !identifier.matches(MULTIPLE_WORD_WILDCARD_REGEX)) {\n            throw new AdminProcessingException(DQPEmbeddedPlugin.Util.getString(\"Admin.Invalid_identifier\")); //$NON-NLS-1$                \n        }\n        \n        List results = new ArrayList();\n        if (matches(identifier, \"dqp\")) { //$NON-NLS-1$\n            // First get the queue statistics for the DQP\n            Collection c = manager.getDQP().getQueueStatistics();;\n            if (c != null && !c.isEmpty()) {\n                results.addAll(c);\n            }\n        }\n        \n        try {\n            // Now get for all the connector bindings\n            Collection bindings = super.getConnectorBindings(identifier);\n            for (Iterator i = bindings.iterator(); i.hasNext();) {\n                ConnectorBinding binding = (ConnectorBinding)i.next();\n                Collection c = getDataService().getConnectorBindingStatistics(binding.getName());\n                if (c != null && !c.isEmpty()) {\n                    results.addAll(c);\n                }                \n            }\n        } catch (MetaMatrixComponentException e) {\n        \tthrow new AdminComponentException(e);\n        }\n                \n        if (!results.isEmpty()) {\n            return (List)convertToAdminObjects(results);\n        }\n        return Collections.EMPTY_LIST;\n    }", "nl_input": "Add ability to monitor Connector Connection Pool \nHere are the discussed changes to enable connection pool monitoring:\n\n-   Create a new stat's class to expose the information: ConnectionPoolStats\n-   Enable the ConnectionPool to monitor the following pieces of information:\n\n\na.   Total Connections  - Total Number of Connections for the Connection Pool\nb   Available Connections  - Number of available connections in the connection pool.\nc   Active Connections - Number of Connections currently supporting clients.\nd   Connections Created - Number of Connections created since the Connection Pool was created.\ne  Connections Destroyed  -     Number of Connections destroyed since the Connection Pool was created. \n\nIn the config.xml for the base Connector component type\n\na.  Rename the ConnectorMaxThreads to MaxConnections so that it\nrelates better to the user\n\n\n\n\n-  Expose the ConnectionPoolStats out the adminAPI\n", "code_len": 368, "nl_len": 202, "path": "runtime/src/main/java/com/metamatrix/dqp/embedded/admin/DQPMonitoringAdminImpl.java"}, {"prediction": 0.9399410486221313, "code": "\t@Override\n\tpublic final void open() throws Exception {\n\t\tsuper.open();\n\n\t\ttimestampedCollector = new TimestampedCollector<>(output);\n\n\t\t// these could already be initialized from restoreState()\n\t\tif (watermarkTimers == null) {\n\t\t\twatermarkTimers = new HashSet<>();\n\t\t\twatermarkTimersQueue = new PriorityQueue<>(100);\n\t\t}\n\t\tif (processingTimeTimers == null) {\n\t\t\tprocessingTimeTimers = new HashSet<>();\n\t\t\tprocessingTimeTimersQueue = new PriorityQueue<>(100);\n\t\t}\n\n\t\tcontext = new Context(null, null);\n\n\t\twindowAssignerContext = new WindowAssigner.WindowAssignerContext() {\n\t\t\t@Override\n\t\t\tpublic long getCurrentProcessingTime() {\n\t\t\t\treturn WindowOperator.this.getProcessingTimeService().getCurrentProcessingTime();\n\t\t\t}\n\t\t};\n\n\t\tif (windowAssigner instanceof MergingWindowAssigner) {\n\t\t\tmergingWindowsByKey = new HashMap<>();\n\t\t}\n\n\t\t// re-register the restored timers (if any)\n\t\tif (processingTimeTimersQueue.size() > 0) {\n\t\t\tnextTimer = getProcessingTimeService().registerTimer(processingTimeTimersQueue.peek().timestamp, this);\n\t\t}\n\t}", "nl_input": "Add an interface for Time aware User Functions\nI suggest to add an interface that UDFs can implement, which will let them be notified upon watermark updates.\n\nExample usage:\n{code}\npublic interface EventTimeFunction {\n    void onWatermark(Watermark watermark);\n}\n\npublic class MyMapper implements MapFunction, EventTimeFunction {\n\n    private long currentEventTime = Long.MIN_VALUE;\n\n    public String map(String value) {\n        return value + \" @ \" + currentEventTime;\n    }\n\n    public void onWatermark(Watermark watermark) {\n        currentEventTime = watermark.getTimestamp();\n    }\n}\n{code}", "code_len": 295, "nl_len": 145, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperator.java"}, {"prediction": 0.9406755566596985, "code": "\tprivate void setStats(ConnectionPool connpool, ConnectionPoolStats stats) {\n\n\t\tstats.setConnectionsWaiting(connpool.getNumberOfConnectinsWaiting());\n\t\tstats.setConnectionsCreated(connpool.getTotalCreatedConnectionCount());\n\t\tstats.setConnectionsDestroyed(connpool.getTotalDestroyedConnectionCount());\n\t\tstats.setConnectionsInUse(connpool.getNumberOfConnectionsInUse());\n\t\tstats.setTotalConnections(connpool.getTotalConnectionCount());\n\t}", "nl_input": "Add ability to monitor Connector Connection Pool \nHere are the discussed changes to enable connection pool monitoring:\n\n-   Create a new stat's class to expose the information: ConnectionPoolStats\n-   Enable the ConnectionPool to monitor the following pieces of information:\n\n\na.   Total Connections  - Total Number of Connections for the Connection Pool\nb   Available Connections  - Number of available connections in the connection pool.\nc   Active Connections - Number of Connections currently supporting clients.\nd   Connections Created - Number of Connections created since the Connection Pool was created.\ne  Connections Destroyed  -     Number of Connections destroyed since the Connection Pool was created. \n\nIn the config.xml for the base Connector component type\n\na.  Rename the ConnectorMaxThreads to MaxConnections so that it\nrelates better to the user\n\n\n\n\n-  Expose the ConnectionPoolStats out the adminAPI\n", "code_len": 104, "nl_len": 202, "path": "engine/src/main/java/org/teiid/dqp/internal/pooling/connector/PooledConnector.java"}, {"prediction": 0.9446905255317688, "code": "\tvoid init(IterationRuntimeContext context) {\n\t\tthis.runtimeContext = context;\n\t\tthis.outValue = new Tuple2<K, Message>();\n\t\tthis.edgeIterator = new EdgesIterator<K, EV>();\n\t}", "nl_input": "Add a Pregel iteration abstraction to Gelly\nThis issue proposes to add a Pregel/Giraph-like iteration abstraction to Gelly that will only expose one UDF to the user, {{compute()}}. {{compute()}} will have access to both the vertex state and the incoming messages, and will be able to produce messages and update the vertex value.", "code_len": 56, "nl_len": 76, "path": "flink-libraries/flink-gelly/src/main/java/org/apache/flink/graph/spargel/MessagingFunction.java"}, {"prediction": 0.9477491974830627, "code": "  @Override\n  public long getCumulativeCpuTime() {\n    for (ProcessInfo p : processTree.values()) {\n      if (cpuTimeMs == UNAVAILABLE) {\n        cpuTimeMs = 0;\n      }\n      cpuTimeMs += p.cpuTimeMsDelta;\n    }\n    return cpuTimeMs;\n  }", "nl_input": "CPU resource monitoring in Windows\nThe current implementation of getCpuUsagePercent() for WindowsBasedProcessTree is left as unavailable. Attached a proposal of how to do it. I reused the CpuTimeTracker using 1 jiffy=1ms.\n\nThis was left open by YARN-3122.", "code_len": 76, "nl_len": 64, "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/WindowsBasedProcessTree.java"}]}