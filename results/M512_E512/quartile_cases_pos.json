{"lowest": [{"prediction": 0.07440989464521408, "code": "  private static void updateFrameworkProperties(final Map<String, String> frameworkProps, final String key, final String value) {\n    if (frameworkProps.containsKey(key)) {\n      if (isListValuedProperty(key)) {\n        // TODO should validate that different values don't conflict\n        final String oldValue = frameworkProps.get(key);\n        final String newValue = oldValue + \" \" + value;\n        log.debug(\"Merging property {} = {}\", key, newValue);\n        frameworkProps.put(key, newValue);\n      } else {\n        log.warn(\"The property {} has been set multiple times.\", key);\n        frameworkProps.put(key, value);\n      }\n    } else {\n      frameworkProps.put(key, value);\n    }\n  }", "nl_input": "Allow package wildcard usage for filterting marshalling classes\nAllow using package wildcard filtering for including and excluding serializable types.\r\n\r\nEx:\r\nerrai.marshalling.serializableTypes=org.foo.client.* \\\r\n                                    org.abcinc.model.client.*\r\n\r\n\r\nerrai.marshalling.nonserializableTypes=org.foo.client.* \\\r\n                                       org.abcinc.model.client.*", "code_len": 174, "nl_len": 97, "path": "errai-config/src/main/java/org/jboss/errai/config/rebind/EnvUtil.java"}, {"prediction": 0.07730592042207718, "code": "  private static boolean isJsonBundle(final String path) {\n    return path.endsWith(\".json\");\n  }", "nl_input": "i18n allow translations in template views to use keys without prefix\nCurrently, data-i18n-prefix is set in templated views, forcing that each translation key to be composed by viewname.key. Problem is that in some cases it can cause keys to be duplicated across different views and translation service. See example below for a common key:\r\n\r\nclose=Close\r\nview1.close=Close\r\nview2.close=Close\r\n\r\nIdeally, we should be able to share the same key (close) across the different views and translationservice.format.\r\n\r\nIt could be that the translation at the template levels would look for prefix at data-i18n-prefix but fallback to no prefix in case that is not found.", "code_len": 25, "nl_len": 162, "path": "errai-ui/src/main/java/org/jboss/errai/ui/rebind/TranslationServiceGenerator.java"}, {"prediction": 0.07876593619585037, "code": "    /**\n     * {@inheritDoc}\n     */\n    @Override\n    public PropertyModel setName(String name) {\n        setModelAttribute(PropertyModel.NAME, name);\n        return this;\n    }", "nl_input": "Add runtime configuration\nWe need to add a runtime configuration to externalize a number of our runtime settings.  Tops on the list for 0.1 is the list of activators used by a Deployer.  The list should be pulled from the runtime config in Deployer.  \n\nSome attention will have to be paid to how this impacts our test setup, which depends on Deployer to register activators.  We will need a default runtime configuration or a way to add activators as part of a MixIn, which actually would be kinda cool.", "code_len": 47, "nl_len": 117, "path": "config/src/main/java/org/switchyard/config/model/domain/v1/V1PropertyModel.java"}, {"prediction": 0.0815449059009552, "code": "    /**\n     * {@inheritDoc}\n     */\n    @Override\n    public String getName() {\n        return getModelAttribute(PropertyModel.NAME);\n    }", "nl_input": "Add runtime configuration\nWe need to add a runtime configuration to externalize a number of our runtime settings.  Tops on the list for 0.1 is the list of activators used by a Deployer.  The list should be pulled from the runtime config in Deployer.  \n\nSome attention will have to be paid to how this impacts our test setup, which depends on Deployer to register activators.  We will need a default runtime configuration or a way to add activators as part of a MixIn, which actually would be kinda cool.", "code_len": 36, "nl_len": 117, "path": "config/src/main/java/org/switchyard/config/model/domain/v1/V1PropertyModel.java"}, {"prediction": 0.08403071761131287, "code": "    /**\n     * {@inheritDoc}\n     */\n    @Override\n    public String getValue() {\n        return getModelAttribute(PropertyModel.VALUE);\n    }", "nl_input": "Add runtime configuration\nWe need to add a runtime configuration to externalize a number of our runtime settings.  Tops on the list for 0.1 is the list of activators used by a Deployer.  The list should be pulled from the runtime config in Deployer.  \n\nSome attention will have to be paid to how this impacts our test setup, which depends on Deployer to register activators.  We will need a default runtime configuration or a way to add activators as part of a MixIn, which actually would be kinda cool.", "code_len": 36, "nl_len": 117, "path": "config/src/main/java/org/switchyard/config/model/domain/v1/V1PropertyModel.java"}], "q1": [{"prediction": 0.3311690092086792, "code": "  /**\n   * Returns str, right-padded with pad to a length of len\n   * For example:\n   *   ('hi', 5, '??') =&gt; 'hi???'\n   *   ('hi', 1, '??') =&gt; 'h'\n   */\n  public UTF8String rpad(int len, UTF8String pad) {\n    int spaces = len - this.numChars(); // number of char need to pad\n    if (spaces <= 0 || pad.numBytes() == 0) {\n      // no padding at all, return the substring of the current string\n      return substring(0, len);\n    } else {\n      int padChars = pad.numChars();\n      int count = spaces / padChars; // how many padding string needed\n      // the partial string of the padding\n      UTF8String remain = pad.substring(0, spaces - padChars * count);\n\n      byte[] data = new byte[this.numBytes + pad.numBytes * count + remain.numBytes];\n      copyMemory(this.base, this.offset, data, BYTE_ARRAY_OFFSET, this.numBytes);\n      int offset = this.numBytes;\n      int idx = 0;\n      while (idx < count) {\n        copyMemory(pad.base, pad.offset, data, BYTE_ARRAY_OFFSET + offset, pad.numBytes);\n        ++ idx;\n        offset += pad.numBytes;\n      }\n      copyMemory(remain.base, remain.offset, data, BYTE_ARRAY_OFFSET + offset, remain.numBytes);\n\n      return UTF8String.fromBytes(data);\n    }\n  }", "nl_input": "Add property-based tests for UTF8String\nUTF8String is a class that can benefit from ScalaCheck-style property checks. Let's add these.", "code_len": 365, "nl_len": 34, "path": "unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java"}, {"prediction": 0.3311842978000641, "code": "      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetRegionInfoResponse other) {\n        if (other == org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetRegionInfoResponse.getDefaultInstance()) return this;\n        if (other.hasRegionInfo()) {\n          mergeRegionInfo(other.getRegionInfo());\n        }\n        if (other.hasCompactionState()) {\n          setCompactionState(other.getCompactionState());\n        }\n        this.mergeUnknownFields(other.getUnknownFields());\n        return this;\n      }", "nl_input": "Adding some fuction to check if a table/region is in compaction\nThis feature will be helpful to find out if a major compaction is going on.\nWe can show if it is in any minor compaction too.", "code_len": 135, "nl_len": 44, "path": "src/main/java/org/apache/hadoop/hbase/protobuf/generated/AdminProtos.java"}, {"prediction": 0.3315558433532715, "code": "    @Override\n    public BatchCommitModel setBatchTimeout(long delay) {\n        setModelAttribute(JCAConstants.BATCH_TIMEOUT, Long.toString(delay));\n        return this;\n    }", "nl_input": "Support batch processing on JCA inbound\nSince JCA message inflow creates a transaction for every message, one project I have got involved before, experienced performance problem on their ESB like framework. We solved it with receiving multiple messages in one transaction.\n\nIt would be great if we add similar to the SwitchYard like following:\nhttps://gist.github.com/3876644\n\nKeith suggested that we may be able to have this as a common component library or core extension so we can use this feature not only on JCA but on other components.", "code_len": 45, "nl_len": 119, "path": "jca/src/main/java/org/switchyard/component/jca/config/model/v1/V1BatchCommitModel.java"}, {"prediction": 0.33234041929244995, "code": "  public void setReversedIsSet(boolean value) {\n    __isset_bitfield = EncodingUtils.setBit(__isset_bitfield, __REVERSED_ISSET_ID, value);\n  }", "nl_input": "Support reverse scan in thrift2\nSupport reverse scan in thrift2.", "code_len": 42, "nl_len": 14, "path": "hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/TScan.java"}, {"prediction": 0.33234041929244995, "code": "  public void setReversedIsSet(boolean value) {\n    __isset_bitfield = EncodingUtils.setBit(__isset_bitfield, __REVERSED_ISSET_ID, value);\n  }", "nl_input": "Support reverse scan in thrift2\nSupport reverse scan in thrift2.", "code_len": 42, "nl_len": 14, "path": "hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/TScan.java"}], "median": [{"prediction": 0.5563368797302246, "code": "  /*\n   * @return table name given archive file path\n   */\n  public static TableName getTableName(Path archivePath) {\n    Path p = archivePath;\n    String tbl = null;\n    // namespace is the 4th parent of file\n    for (int i = 0; i < 5; i++) {\n      if (p == null) return null;\n      if (i == 3) tbl = p.getName();\n      p = p.getParent();\n    }\n    if (p == null) return null;\n    return TableName.valueOf(p.getName(), tbl);\n  }", "nl_input": "Incremental backup and bulk loading\nCurrently, incremental backup is based on WAL files. Bulk data loading bypasses WALs for obvious reasons, breaking incremental backups. The only way to continue backups after bulk loading is to create new full backup of a table. This may not be feasible for customers who do bulk loading regularly (say, every day).\n\nHere is the review board (out of date):\nhttps://reviews.apache.org/r/54258/\n\nIn order not to miss the hfiles which are loaded into region directories in a situation where postBulkLoadHFile() hook is not called (bulk load being interrupted), we record hfile names thru preCommitStoreFile() hook.\nAt time of incremental backup, we check the presence of such hfiles. If they are present, they become part of the incremental backup image.\n\nHere is review board:\nhttps://reviews.apache.org/r/57790/\n\nGoogle doc for design:\nhttps://docs.google.com/document/d/1ACCLsecHDvzVSasORgqqRNrloGx4mNYIbvAU7lq5lJE", "code_len": 136, "nl_len": 249, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/HFileArchiveUtil.java"}, {"prediction": 0.5563520789146423, "code": "  /** Get the event type */\n  public Events.EventType getEventType() {\n    if (\"FAILED\".equals(getStatus())) {\n      return Events.EventType.JOB_FAILED;\n    } else\n      return Events.EventType.JOB_KILLED;\n  }", "nl_input": "Modify JobHistory to use Avro for serialization instead of raw JSON\nMAPREDUCE-157 modifies JobHistory to log events using Json Format.  This can be modified to use Avro instead. ", "code_len": 58, "nl_len": 42, "path": "src/java/org/apache/hadoop/mapreduce/jobhistory/JobUnsuccessfulCompletionEvent.java"}, {"prediction": 0.5565729737281799, "code": "    public void setAmount(double amount) {\n        this.amount = amount;\n    }", "nl_input": "Support remote transaction propagation via SCA binding\nAdd remote transaction propagation on SCA binding so the transaction is propagated to remote SwitchYard node.", "code_len": 21, "nl_len": 29, "path": "demos/transaction-propagation/dealer/src/main/java/org/switchyard/quickstarts/demo/txpropagation/Offer.java"}, {"prediction": 0.5566225647926331, "code": "  /**\n   * Get the total pool size that is available for ORC writers.\n   * @return the number of bytes in the pool\n   */\n  long getTotalMemoryPool() {\n    return totalMemoryPool;\n  }", "nl_input": "Implement a memory manager for ORC\nWith the large default stripe size (256MB) and dynamic partitions, it is quite easy for users to run out of memory when writing ORC files. We probably need a solution that keeps track of the total number of concurrent ORC writers and divides the available heap space between them. ", "code_len": 50, "nl_len": 68, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/MemoryManager.java"}, {"prediction": 0.5566841959953308, "code": "  /** Get the tracker name */\n  public String getTrackerName() { return datum.trackerName.toString(); }", "nl_input": "Modify JobHistory to use Avro for serialization instead of raw JSON\nMAPREDUCE-157 modifies JobHistory to log events using Json Format.  This can be modified to use Avro instead. ", "code_len": 25, "nl_len": 42, "path": "src/java/org/apache/hadoop/mapreduce/jobhistory/TaskAttemptStartedEvent.java"}], "q3": [{"prediction": 0.751997172832489, "code": "  /**\n   * Should only be called by bootstrapper for testing purposes.\n   */\n  public void __resetSubsystem() {\n    for (final String eventType : new HashSet<String>(((ClientMessageBus) ErraiBus.get()).getAllRegisteredSubjects())) {\n      if (eventType.startsWith(CDI_SUBJECT_PREFIX)) {\n        ErraiBus.get().unsubscribeAll(eventType);\n      }\n    }\n\n    remoteEvents.clear();\n    active = false;\n    fireOnSubscribe.clear();\n    eventObservers.clear();\n    localObserverTypes.clear();\n    lookupTable = Collections.emptyMap();\n  }", "nl_input": "Add client only CDI event support\nCDI events are pretty nice. Anyhow they are currently designed for sending events over the wire. So for every @Portable annotated class used as an event there is a remote subscribed message send to the server during the startup of the app. This is even the case for events only used on the client side but annotated with @Portable because of the needed Errai RPC support.\r\n\r\nThere should be an option (e.g. additional annotation) to specify which classes are used for client-server events. \r\n\r\nSee this discussion for some more details: https://community.jboss.org/thread/232658", "code_len": 146, "nl_len": 142, "path": "errai-cdi/errai-cdi-client/src/main/java/org/jboss/errai/enterprise/client/cdi/api/CDI.java"}, {"prediction": 0.7520463466644287, "code": "    public void directoryWalkStep( int percentage, File file )\n    {\n        log.debug( \"Walk Step: \" + percentage + \", \" + file );\n\n        stats.increaseFileCount();\n\n        // consume files regardless - the predicate will check the timestamp\n        BaseFile basefile = new BaseFile( repository.getLocation(), file );\n        \n        // Timestamp finished points to the last successful scan, not this current one.\n        if ( file.lastModified() >= changesSince )\n        {\n            stats.increaseNewFileCount();\n            newFiles.add( basefile );       \n        }\n        \n        consumerProcessFile.setBasefile( basefile );\n        consumerWantsFile.setBasefile( basefile );\n        \n        Closure processIfWanted = IfClosure.getInstance( consumerWantsFile, consumerProcessFile );\n        CollectionUtils.forAllDo( this.knownConsumers, processIfWanted );\n        \n        if ( consumerWantsFile.getWantedFileCount() <= 0 )\n        {\n            // Nothing known processed this file.  It is invalid!\n            CollectionUtils.forAllDo( this.invalidConsumers, consumerProcessFile );\n        }\n    }", "nl_input": "add RSS view to repository manager\npossibly needs a new component in JIRA. Items that could have RSS:\n- a particular search\n- preset for \"latest added\" artifacts\n- preset for \"new versions\" of artifacts\n- preset for \"new artifacts from a given sync partner\"", "code_len": 258, "nl_len": 60, "path": "archiva-modules/archiva-base/archiva-repository-layer/src/main/java/org/apache/maven/archiva/repository/scanner/RepositoryScannerInstance.java"}, {"prediction": 0.7521417140960693, "code": "  public Configuration getConf() {\n    return conf;\n  }", "nl_input": "Integrate Snappy compression\nGoogle release Zippy as an open source (APLv2) project called Snappy (http://code.google.com/p/snappy). This tracks integrating it into Hadoop.\n\n{quote}\nSnappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. For instance, compared to the fastest mode of zlib, Snappy is an order of magnitude faster for most inputs, but the resulting compressed files are anywhere from 20% to 100% bigger. On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.\n{quote}", "code_len": 15, "nl_len": 176, "path": "common/src/java/org/apache/hadoop/io/compress/SnappyCodec.java"}, {"prediction": 0.7521839141845703, "code": "    protected String getSnapshotArtifactRepositoryPath( Artifact artifact, Snapshot snapshot )\n    {\n        File f = new File( repository.getBasedir(), repository.pathOf( artifact ) );\n        String snapshotInfo = artifact.getVersion().replaceFirst( \"SNAPSHOT\", snapshot.getTimestamp() + \"-\" + \n                                                                            snapshot.getBuildNumber() + \".pom\" );\n        File snapshotFile = new File( f.getParentFile(), artifact.getArtifactId() + \"-\" + snapshotInfo );\n        return snapshotFile.getAbsolutePath();\n    }", "nl_input": "caching repository query interface\nneed to be able to query the repository to see if artifacts exist, what versions are available, etc. This needs to interoperate with the indexing, and where applicable should cache information, balancing the need to keep a low memory overhead but avoid repetitive disk reads on metadata files.", "code_len": 113, "nl_len": 65, "path": "maven-repository-reports-standard/src/main/java/org/apache/maven/repository/reporting/AbstractRepositoryQueryLayer.java"}, {"prediction": 0.7524499297142029, "code": "    /**\n     * {@inheritDoc}\n     */\n    public void doDelete(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        handle(request, response);\n    }", "nl_input": "Basic HTTP Gateway\nCreate support for a basic HTTP gateway.", "code_len": 40, "nl_len": 12, "path": "http/src/main/java/org/switchyard/component/http/HttpGatewayServlet.java"}], "highest": [{"prediction": 0.965755820274353, "code": "  public Class<? extends Compressor> getCompressorType() {\n    if (!isNativeSnappyLoaded(conf)) {\n      throw new RuntimeException(\"native-snappy library not available\");\n    }\n    return SnappyCompressor.class;\n  }", "nl_input": "Integrate Snappy compression\nGoogle release Zippy as an open source (APLv2) project called Snappy (http://code.google.com/p/snappy). This tracks integrating it into Hadoop.\n\n{quote}\nSnappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. For instance, compared to the fastest mode of zlib, Snappy is an order of magnitude faster for most inputs, but the resulting compressed files are anywhere from 20% to 100% bigger. On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.\n{quote}", "code_len": 55, "nl_len": 176, "path": "common/src/java/org/apache/hadoop/io/compress/SnappyCodec.java"}, {"prediction": 0.965756356716156, "code": "  public Class<? extends Decompressor> getDecompressorType() {\n    if (!isNativeSnappyLoaded(conf)) {\n      throw new RuntimeException(\"native-snappy library not available\");\n    }\n    return SnappyDecompressor.class;\n  }", "nl_input": "Integrate Snappy compression\nGoogle release Zippy as an open source (APLv2) project called Snappy (http://code.google.com/p/snappy). This tracks integrating it into Hadoop.\n\n{quote}\nSnappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. For instance, compared to the fastest mode of zlib, Snappy is an order of magnitude faster for most inputs, but the resulting compressed files are anywhere from 20% to 100% bigger. On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.\n{quote}", "code_len": 57, "nl_len": 176, "path": "common/src/java/org/apache/hadoop/io/compress/SnappyCodec.java"}, {"prediction": 0.9672122597694397, "code": "  public CompressionInputStream createInputStream(InputStream in,\n      Decompressor decompressor) throws IOException {\n    if (!isNativeSnappyLoaded(conf)) {\n      throw new RuntimeException(\"native-snappy library not available\");\n    }\n    return new BlockDecompressorStream(in, decompressor, conf.getInt(\n        SNAPPY_BUFFER_SIZE_KEY, DEFAULT_SNAPPY_BUFFER_SIZE));\n  }", "nl_input": "Integrate Snappy compression\nGoogle release Zippy as an open source (APLv2) project called Snappy (http://code.google.com/p/snappy). This tracks integrating it into Hadoop.\n\n{quote}\nSnappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. For instance, compared to the fastest mode of zlib, Snappy is an order of magnitude faster for most inputs, but the resulting compressed files are anywhere from 20% to 100% bigger. On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.\n{quote}", "code_len": 93, "nl_len": 176, "path": "common/src/java/org/apache/hadoop/io/compress/SnappyCodec.java"}, {"prediction": 0.9678896069526672, "code": "  public String getDefaultExtension() {\n    return \".snappy\";\n  }", "nl_input": "Integrate Snappy compression\nGoogle release Zippy as an open source (APLv2) project called Snappy (http://code.google.com/p/snappy). This tracks integrating it into Hadoop.\n\n{quote}\nSnappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. For instance, compared to the fastest mode of zlib, Snappy is an order of magnitude faster for most inputs, but the resulting compressed files are anywhere from 20% to 100% bigger. On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.\n{quote}", "code_len": 17, "nl_len": 176, "path": "common/src/java/org/apache/hadoop/io/compress/SnappyCodec.java"}, {"prediction": 0.9680073857307434, "code": "  public CompressionOutputStream createOutputStream(OutputStream out,\n      Compressor compressor) throws IOException {\n\n    if (!isNativeSnappyLoaded(conf)) {\n      throw new RuntimeException(\"native-snappy library not available\");\n    }\n\n    int bufferSize = conf.getInt(SNAPPY_BUFFER_SIZE_KEY,\n        DEFAULT_SNAPPY_BUFFER_SIZE);\n\n    int compressionOverhead = Snappy.maxCompressedLength(bufferSize) - bufferSize; \n\n    return new BlockCompressorStream(out, compressor, bufferSize,\n        compressionOverhead);\n  }", "nl_input": "Integrate Snappy compression\nGoogle release Zippy as an open source (APLv2) project called Snappy (http://code.google.com/p/snappy). This tracks integrating it into Hadoop.\n\n{quote}\nSnappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. For instance, compared to the fastest mode of zlib, Snappy is an order of magnitude faster for most inputs, but the resulting compressed files are anywhere from 20% to 100% bigger. On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.\n{quote}", "code_len": 126, "nl_len": 176, "path": "common/src/java/org/apache/hadoop/io/compress/SnappyCodec.java"}]}