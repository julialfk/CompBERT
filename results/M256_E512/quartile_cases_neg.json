{"lowest": [{"prediction": 0.07685732841491699, "code": "    /**\n     * Method to retrieve The FileUtils, which is shared by all users of this\n     * method.\n     *\n     * @return an instance of FileUtils.\n     */\n    public static FileUtils getFileUtils()\n    {\n        return PRIMARY_INSTANCE;\n    }", "nl_input": "Support the creation of a temporary directory at install time which is cleaned up when install completes\nA temporary directory which can be referenced in the install.xml would make the management of files needed at install time much simpler. The attached patch (to the git head as of 2010/08/24) adds support for one or more temporary directories which are automatically cleaned up when the installer completes. To aid in debugging temporary directories are not deleted when tracing is enabled.", "code_len": 61, "nl_len": 91, "path": "izpack-util/src/main/java/com/izforge/izpack/util/file/FileUtils.java"}, {"prediction": 0.07845933735370636, "code": "    /**\n     * Convenience method to copy a file from a source to a destination.\n     * No filtering is performed.\n     *\n     * @param sourceFile the file to copy from.\n     *                   Must not be <code>null</code>.\n     * @param destFile   the file to copy to.\n     *                   Must not be <code>null</code>.\n     * @throws IOException if the copying fails.\n     */\n    public void copyFile(File sourceFile, File destFile) throws IOException\n    {\n        copyFile(sourceFile, destFile, false, false);\n    }", "nl_input": "Support the creation of a temporary directory at install time which is cleaned up when install completes\nA temporary directory which can be referenced in the install.xml would make the management of files needed at install time much simpler. The attached patch (to the git head as of 2010/08/24) adds support for one or more temporary directories which are automatically cleaned up when the installer completes. To aid in debugging temporary directories are not deleted when tracing is enabled.", "code_len": 134, "nl_len": 91, "path": "izpack-util/src/main/java/com/izforge/izpack/util/file/FileUtils.java"}, {"prediction": 0.0788949504494667, "code": "    /**\n     * Gets the version.\n     *\n     * @return The application version.\n     */\n    public String getAppVersion()\n    {\n        return appVersion;\n    }", "nl_input": "Support the creation of a temporary directory at install time which is cleaned up when install completes\nA temporary directory which can be referenced in the install.xml would make the management of files needed at install time much simpler. The attached patch (to the git head as of 2010/08/24) adds support for one or more temporary directories which are automatically cleaned up when the installer completes. To aid in debugging temporary directories are not deleted when tracing is enabled.", "code_len": 43, "nl_len": 91, "path": "izpack-api/src/main/java/com/izforge/izpack/api/data/Info.java"}, {"prediction": 0.0807366818189621, "code": "    /**\n     * Gets the application name.\n     *\n     * @return The application name.\n     */\n    public String getAppName()\n    {\n        return appName;\n    }", "nl_input": "Support the creation of a temporary directory at install time which is cleaned up when install completes\nA temporary directory which can be referenced in the install.xml would make the management of files needed at install time much simpler. The attached patch (to the git head as of 2010/08/24) adds support for one or more temporary directories which are automatically cleaned up when the installer completes. To aid in debugging temporary directories are not deleted when tracing is enabled.", "code_len": 43, "nl_len": 91, "path": "izpack-api/src/main/java/com/izforge/izpack/api/data/Info.java"}, {"prediction": 0.08136896789073944, "code": "    /**\n     * Sets the version.\n     *\n     * @param appVersion The application version.\n     */\n    public void setAppVersion(String appVersion)\n    {\n        this.appVersion = appVersion;\n    }", "nl_input": "Support the creation of a temporary directory at install time which is cleaned up when install completes\nA temporary directory which can be referenced in the install.xml would make the management of files needed at install time much simpler. The attached patch (to the git head as of 2010/08/24) adds support for one or more temporary directories which are automatically cleaned up when the installer completes. To aid in debugging temporary directories are not deleted when tracing is enabled.", "code_len": 54, "nl_len": 91, "path": "izpack-api/src/main/java/com/izforge/izpack/api/data/Info.java"}], "q1": [{"prediction": 0.29113826155662537, "code": "    public void addToPart_vals(String elem) {\n      if (this.part_vals == null) {\n        this.part_vals = new ArrayList<String>();\n      }\n      this.part_vals.add(elem);\n    }", "nl_input": "Passing user identity from metastore client to server in non-secure mode\nCurrently in unsecure mode client don't pass on user identity. As a result hdfs and other operations done by server gets executed by user running metastore process instead of being done in context of client. This results in problem as reported here: \nhttp://mail-archives.apache.org/mod_mbox/hive-user/201111.mbox/%3CCAK0mCrRC3aPqtRHDe2J25Rm0JX6TS1KXxd7KPjqJjoqBjg=APA@mail.gmail.com%3E", "code_len": 56, "nl_len": 137, "path": "metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java"}, {"prediction": 0.2911565899848938, "code": "      public void write_args(org.apache.thrift.protocol.TProtocol prot) throws org.apache.thrift.TException {\n        prot.writeMessageBegin(new org.apache.thrift.protocol.TMessage(\"append_partition_by_name\", org.apache.thrift.protocol.TMessageType.CALL, 0));\n        append_partition_by_name_args args = new append_partition_by_name_args();\n        args.setDb_name(db_name);\n        args.setTbl_name(tbl_name);\n        args.setPart_name(part_name);\n        args.write(prot);\n        prot.writeMessageEnd();\n      }", "nl_input": "Passing user identity from metastore client to server in non-secure mode\nCurrently in unsecure mode client don't pass on user identity. As a result hdfs and other operations done by server gets executed by user running metastore process instead of being done in context of client. This results in problem as reported here: \nhttp://mail-archives.apache.org/mod_mbox/hive-user/201111.mbox/%3CCAK0mCrRC3aPqtRHDe2J25Rm0JX6TS1KXxd7KPjqJjoqBjg=APA@mail.gmail.com%3E", "code_len": 154, "nl_len": 137, "path": "metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java"}, {"prediction": 0.29115718603134155, "code": "    private void initFields() {\n      regionToFlush_ = java.util.Collections.emptyList();;\n    }", "nl_input": "Adding some fuction to check if a table/region is in compaction\nThis feature will be helpful to find out if a major compaction is going on.\nWe can show if it is in any minor compaction too.", "code_len": 26, "nl_len": 44, "path": "src/main/java/org/apache/hadoop/hbase/protobuf/generated/AdminProtos.java"}, {"prediction": 0.2911607623100281, "code": "    public void create_role(Role role, org.apache.thrift.async.AsyncMethodCallback<create_role_call> resultHandler) throws org.apache.thrift.TException {\n      checkReady();\n      create_role_call method_call = new create_role_call(role, resultHandler, this, ___protocolFactory, ___transport);\n      this.___currentMethod = method_call;\n      ___manager.call(method_call);\n    }", "nl_input": "Passing user identity from metastore client to server in non-secure mode\nCurrently in unsecure mode client don't pass on user identity. As a result hdfs and other operations done by server gets executed by user running metastore process instead of being done in context of client. This results in problem as reported here: \nhttp://mail-archives.apache.org/mod_mbox/hive-user/201111.mbox/%3CCAK0mCrRC3aPqtRHDe2J25Rm0JX6TS1KXxd7KPjqJjoqBjg=APA@mail.gmail.com%3E", "code_len": 104, "nl_len": 137, "path": "metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java"}, {"prediction": 0.2911668121814728, "code": "    public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry parseFrom(\n        com.google.protobuf.ByteString data,\n        com.google.protobuf.ExtensionRegistryLite extensionRegistry)\n        throws com.google.protobuf.InvalidProtocolBufferException {\n      return newBuilder().mergeFrom(data, extensionRegistry)\n               .buildParsed();\n    }", "nl_input": "Adding some fuction to check if a table/region is in compaction\nThis feature will be helpful to find out if a major compaction is going on.\nWe can show if it is in any minor compaction too.", "code_len": 86, "nl_len": 44, "path": "src/main/java/org/apache/hadoop/hbase/protobuf/generated/AdminProtos.java"}], "median": [{"prediction": 0.394438236951828, "code": "  // TODO: perhaps return the version number from this write?\n  public boolean writeZNode(String znodeName, byte[] data, int version, boolean watch) throws IOException {\n      try {\n        String fullyQualifiedZNodeName = getZNode(parentZNode, znodeName);\n        zooKeeper.setData(fullyQualifiedZNodeName, data, version);\n        if(watch) {\n          zooKeeper.getData(fullyQualifiedZNodeName, this, null);\n        }\n        return true;\n      } catch (InterruptedException e) {\n        LOG.warn(\"<\" + instanceName + \">\" + \"Failed to write data to ZooKeeper\", e);\n        throw new IOException(e);\n      } catch (KeeperException e) {\n        LOG.warn(\"<\" + instanceName + \">\" + \"Failed to write data to ZooKeeper\", e);\n        throw new IOException(e);\n      }\n    }", "nl_input": "Add Backup CLI Option to HMaster\nThe HMaster main() should allow a toggle like --backup, which forces it to be a secondary master on startup versus a primary candidate.  That way, we can start up multiple masters at once and deterministically know which one will be the original primary.", "code_len": 201, "nl_len": 62, "path": "src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java"}, {"prediction": 0.3944401741027832, "code": "  @SuppressWarnings(\"nls\")\n  public void doPhase1QBExpr(ASTNode ast, QBExpr qbexpr, String id, String alias)\n      throws SemanticException {\n\n    assert (ast.getToken() != null);\n    switch (ast.getToken().getType()) {\n    case HiveParser.TOK_QUERY: {\n      QB qb = new QB(id, alias, true);\n      doPhase1(ast, qb, initPhase1Ctx());\n      qbexpr.setOpcode(QBExpr.Opcode.NULLOP);\n      qbexpr.setQB(qb);\n    }\n      break;\n    case HiveParser.TOK_UNION: {\n      qbexpr.setOpcode(QBExpr.Opcode.UNION);\n      // query 1\n      assert (ast.getChild(0) != null);\n      QBExpr qbexpr1 = new QBExpr(alias + \"-subquery1\");\n      doPhase1QBExpr((ASTNode) ast.getChild(0), qbexpr1, id + \"-subquery1\",\n          alias + \"-subquery1\");\n      qbexpr.setQBExpr1(qbexpr1);\n\n      // query 2\n      assert (ast.getChild(0) != null);\n      QBExpr qbexpr2 = new QBExpr(alias + \"-subquery2\");\n      doPhase1QBExpr((ASTNode) ast.getChild(1), qbexpr2, id + \"-subquery2\",\n          alias + \"-subquery2\");\n      qbexpr.setQBExpr2(qbexpr2);\n    }\n      break;\n    }\n  }", "nl_input": "sorted merge join\nIf the input tables are sorted on the join key, and a mapjoin is being performed, it is useful to exploit the sorted properties of the table.\nThis can lead to substantial cpu savings - this needs to work across bucketed map joins also.\n\nSince, sorted properties of a table are not enforced currently, a new parameter can be added to specify to use the sort-merge join.\n", "code_len": 365, "nl_len": 90, "path": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java"}, {"prediction": 0.3944491446018219, "code": "  /**\n   * @param name\n   * @param dbname\n   * @throws NoSuchObjectException\n   * @throws ExistingDependentsException\n   * @throws MetaException\n   * @throws TException\n   * @see org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.Iface#drop_table(java.lang.String,\n   *      java.lang.String, boolean)\n   */\n  public void dropTable(String dbname, String name)\n      throws NoSuchObjectException, MetaException, TException {\n    dropTable(dbname, name, true, true, null);\n  }", "nl_input": "Enable QOP configuration for Hive Server 2 thrift transport\nThe QoP for hive server 2 should be configurable to enable encryption. A new configuration should be exposed \"hive.server2.thrift.sasl.qop\". This would give greater control configuring hive server 2 service.", "code_len": 144, "nl_len": 57, "path": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java"}, {"prediction": 0.3944567143917084, "code": "    @Override\n    public void visit(WithItem obj) {}", "nl_input": "Support Lateral join and procedure pushdown\nLateral joins of the form:\n\nselect ... from x inner join lateral (... x.col ...) as y on ...\n\nHave been supported for some time, but not the ability to push them to source.\n\nA highly related scenario is to be able to push procedures used directly or in lateral joins with the rest of their plan:\n\nselect ... from x inner join lateral (call proc(.. x.col ...)) as y on ...", "code_len": 14, "nl_len": 109, "path": "api/src/main/java/org/teiid/language/visitor/AbstractLanguageVisitor.java"}, {"prediction": 0.3944579064846039, "code": "\torg.teiid.language.Expression translate(Constant constant) {\n    \tif (constant.isMultiValued()) {\n    \t\tParameter result = new Parameter();\n    \t\tresult.setType(constant.getType());\n    \t\tfinal List<?> values = (List<?>)constant.getValue();\n    \t\tallValues.add(values);\n    \t\tresult.setValueIndex(valueIndex++);\n    \t\treturn result;\n    \t}\n    \tif (constant.getValue() instanceof ArrayImpl) {\n    \t\t//TODO: we could check if there is a common base type (also needs to be in the dependent logic)\n    \t\t// and expand binding options in the translators\n\n    \t\t//we currently support the notion of a mixed type array, since we consider object a common base type\n    \t\t//that will not work for all sources, so instead of treating this as a single array (as commented out below),\n    \t\t//we just turn it into an array of parameters\n    \t\t//Literal result = new Literal(av.getValues(), org.teiid.language.Array.class);\n    \t\t//result.setBindEligible(constant.isBindEligible());\n            //return result;\n\n    \t\tArrayImpl av = (ArrayImpl)constant.getValue();\n    \t\tList<Constant> vals = new ArrayList<Constant>();\n    \t\tClass<?> baseType = null;\n    \t\tfor (Object o : av.getValues()) {\n    \t\t\tConstant c = new Constant(o);\n    \t\t\tc.setBindEligible(constant.isBindEligible());\n    \t\t\tvals.add(c);\n    \t\t\tif (baseType == null) {\n    \t\t\t\tbaseType = c.getType();\n    \t\t\t} else if (!baseType.equals(c.getType())) {\n\t\t\t\t\tbaseType = DataTypeManager.DefaultDataClasses.OBJECT;\n\t\t\t\t}\n    \t\t}\n    \t\treturn new org.teiid.language.Array(baseType, translateExpressionList(vals));   \t\t\n    \t}\n        Literal result = new Literal(constant.getValue(), constant.getType());\n        result.setBindEligible(constant.isBindEligible());\n        return result;\n    }", "nl_input": "Support Lateral join and procedure pushdown\nLateral joins of the form:\n\nselect ... from x inner join lateral (... x.col ...) as y on ...\n\nHave been supported for some time, but not the ability to push them to source.\n\nA highly related scenario is to be able to push procedures used directly or in lateral joins with the rest of their plan:\n\nselect ... from x inner join lateral (call proc(.. x.col ...)) as y on ...", "code_len": 473, "nl_len": 109, "path": "engine/src/main/java/org/teiid/dqp/internal/datamgr/LanguageBridgeFactory.java"}], "q3": [{"prediction": 0.5229027271270752, "code": "    @Override\n    public boolean equals(Object o) {\n      return true;\n    }", "nl_input": "[replication] Add the ability to enable/disable streams\nThis jira was initially in the scope of HBASE-2201, but was pushed out since it has low value compared to the required effort (and when want to ship 0.90.0 rather soonish).\n\nWe need to design a way to enable/disable replication streams in a determinate fashion.", "code_len": 21, "nl_len": 77, "path": "src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java"}, {"prediction": 0.522948682308197, "code": "    private void finish(boolean commit) throws Exception {\n        try {\n            endTransaction(commit);\n        } finally {\n            resetContextClassLoader();\n            releaseThreadLock();\n        }\n    }", "nl_input": "Support batch processing on JCA inbound\nSince JCA message inflow creates a transaction for every message, one project I have got involved before, experienced performance problem on their ESB like framework. We solved it with receiving multiple messages in one transaction.\n\nIt would be great if we add similar to the SwitchYard like following:\nhttps://gist.github.com/3876644\n\nKeith suggested that we may be able to have this as a common component library or core extension so we can use this feature not only on JCA but on other components.", "code_len": 45, "nl_len": 119, "path": "jca/src/main/java/org/switchyard/component/jca/EndpointProxy.java"}, {"prediction": 0.5229840278625488, "code": "    @Override\n    public boolean isPartitionMarkedForEvent(final String db_name, final String tbl_name,\n        final Map<String,String> partName, final PartitionEventType evtType) throws\n        MetaException, NoSuchObjectException, UnknownDBException, UnknownTableException,\n        TException, UnknownPartitionException, InvalidPartitionException {\n\n      startPartitionFunction(\"isPartitionMarkedForEvent\", db_name, tbl_name, partName);\n      Boolean ret = null;\n      try {\n        ret = executeWithRetry(new Command<Boolean>(){\n          @Override\n          public Boolean run(RawStore ms) throws Exception {\n            return ms.isPartitionMarkedForEvent(db_name, tbl_name, partName, evtType);\n          }\n\n        });\n      } catch (Exception original) {\n        LOG.error(original);\n        if (original instanceof NoSuchObjectException) {\n          throw (NoSuchObjectException)original;\n        } else if(original instanceof UnknownTableException){\n          throw (UnknownTableException)original;\n        } else if(original instanceof UnknownDBException){\n          throw (UnknownDBException)original;\n        } else if(original instanceof UnknownPartitionException){\n          throw (UnknownPartitionException)original;\n        } else if(original instanceof InvalidPartitionException){\n          throw (InvalidPartitionException)original;\n        } else if(original instanceof MetaException){\n          throw (MetaException)original;\n        } else{\n          MetaException me = new MetaException(original.toString());\n          me.initCause(original);\n          throw me;\n        }\n      }\n      finally{\n        endFunction(\"isPartitionMarkedForEvent\", ret != null);\n      }\n\n      return ret;\n    }", "nl_input": "Passing user identity from metastore client to server in non-secure mode\nCurrently in unsecure mode client don't pass on user identity. As a result hdfs and other operations done by server gets executed by user running metastore process instead of being done in context of client. This results in problem as reported here: \nhttp://mail-archives.apache.org/mod_mbox/hive-user/201111.mbox/%3CCAK0mCrRC3aPqtRHDe2J25Rm0JX6TS1KXxd7KPjqJjoqBjg=APA@mail.gmail.com%3E", "code_len": 388, "nl_len": 137, "path": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java"}, {"prediction": 0.5229966640472412, "code": "  public List<Partition> getPartitionsByNames(String db_name, String tbl_name,\n      List<String> part_names) throws NoSuchObjectException, MetaException, TException {\n    return deepCopyPartitions(client.get_partitions_by_names(db_name, tbl_name, part_names));\n  }", "nl_input": "Enable QOP configuration for Hive Server 2 thrift transport\nThe QoP for hive server 2 should be configurable to enable encryption. A new configuration should be exposed \"hive.server2.thrift.sasl.qop\". This would give greater control configuring hive server 2 service.", "code_len": 74, "nl_len": 57, "path": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java"}, {"prediction": 0.5230126976966858, "code": "    @Override\n    public void visit(XMLNamespaces obj) {\n    \tmarkInvalid(obj, \"Pushdown of XMLNamespaces not allowed\"); //$NON-NLS-1$\n    }", "nl_input": "RF: Add the ability to use DISTINCT in windowed aggregates in EDS\nAccording to the following page: http://docs.jboss.org/teiid/7.5.0.Final/reference/en-US/html_single/#window_functions\n\nThis states that \"windowed aggregates may not use DISTINCT\". I would like to request that use of DISTINCT in aggregate functions be  supported in a future release.\n", "code_len": 41, "nl_len": 93, "path": "engine/src/main/java/org/teiid/query/optimizer/relational/rules/CriteriaCapabilityValidatorVisitor.java"}], "highest": [{"prediction": 0.9236670136451721, "code": "\t@Override\n\tpublic void processWatermark1(Watermark mark) throws Exception {\n\t\tinput1Watermark = mark.getTimestamp();\n\t\tlong newMin = Math.min(input1Watermark, input2Watermark);\n\t\tif (newMin > combinedWatermark) {\n\t\t\tcombinedWatermark = newMin;\n\t\t\toutput.emitWatermark(new Watermark(combinedWatermark));\n\t\t}\n\t}", "nl_input": "Add an interface for Time aware User Functions\nI suggest to add an interface that UDFs can implement, which will let them be notified upon watermark updates.\n\nExample usage:\n{code}\npublic interface EventTimeFunction {\n    void onWatermark(Watermark watermark);\n}\n\npublic class MyMapper implements MapFunction, EventTimeFunction {\n\n    private long currentEventTime = Long.MIN_VALUE;\n\n    public String map(String value) {\n        return value + \" @ \" + currentEventTime;\n    }\n\n    public void onWatermark(Watermark watermark) {\n        currentEventTime = watermark.getTimestamp();\n    }\n}\n{code}", "code_len": 90, "nl_len": 145, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/co/CoStreamMap.java"}, {"prediction": 0.9236670136451721, "code": "\t@Override\n\tpublic void processWatermark1(Watermark mark) throws Exception {\n\t\tinput1Watermark = mark.getTimestamp();\n\t\tlong newMin = Math.min(input1Watermark, input2Watermark);\n\t\tif (newMin > combinedWatermark) {\n\t\t\tcombinedWatermark = newMin;\n\t\t\toutput.emitWatermark(new Watermark(combinedWatermark));\n\t\t}\n\t}", "nl_input": "Add an interface for Time aware User Functions\nI suggest to add an interface that UDFs can implement, which will let them be notified upon watermark updates.\n\nExample usage:\n{code}\npublic interface EventTimeFunction {\n    void onWatermark(Watermark watermark);\n}\n\npublic class MyMapper implements MapFunction, EventTimeFunction {\n\n    private long currentEventTime = Long.MIN_VALUE;\n\n    public String map(String value) {\n        return value + \" @ \" + currentEventTime;\n    }\n\n    public void onWatermark(Watermark watermark) {\n        currentEventTime = watermark.getTimestamp();\n    }\n}\n{code}", "code_len": 90, "nl_len": 145, "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/co/CoStreamFlatMap.java"}, {"prediction": 0.9254157543182373, "code": "    public SearchResults getResults()\n    {\n        return results;\n    }", "nl_input": "Searching within search results\nNULL", "code_len": 18, "nl_len": 6, "path": "archiva-modules/archiva-web/archiva-webapp/src/main/java/org/apache/maven/archiva/web/action/SearchAction.java"}, {"prediction": 0.9411199688911438, "code": "\t@Override\n    /**\n     * Get string for display purposes \n     * @see java.lang.Object#toString()\n     * @since 6.1\n     */\n    public String toString() {\n        StringBuffer str = new StringBuffer();\n        \n        str.append(this.getIdentifier() + \" ConnectionPoolStats:\\n\"); //$NON-NLS-1$\n        str.append(\"\\tisXAPoolType = \" + isXAPoolType()); //$NON-NLS-1$\n        str.append(\"\\ttotalConnections = \" + this.totalConnections); //$NON-NLS-1$\n        str.append(\"\\tinUseConnections = \" + this.connectionInUse); //$NON-NLS-1$\n        str.append(\"\\twaitingConnections = \" + connectionsWaiting);     //$NON-NLS-1$\n        str.append(\"\\tconnectionsCreated = \" + connectionsCreated);     //$NON-NLS-1$\n        str.append(\"\\tconnectionsDestroyed = \" + connectionsDestroyed);     //$NON-NLS-1$\n        return str.toString();\n    }", "nl_input": "Add ability to monitor Connector Connection Pool \nHere are the discussed changes to enable connection pool monitoring:\n\n-   Create a new stat's class to expose the information: ConnectionPoolStats\n-   Enable the ConnectionPool to monitor the following pieces of information:\n\n\na.   Total Connections  - Total Number of Connections for the Connection Pool\nb   Available Connections  - Number of available connections in the connection pool.\nc   Active Connections - Number of Connections currently supporting clients.\nd   Connections Created - Number of Connections created since the Connection Pool was created.\ne  Connections Destroyed  -     Number of Connections destroyed since the Connection Pool was created. \n\nIn the config.xml for the base Connector component type\n\na.  Rename the ConnectorMaxThreads to MaxConnections so that it\nrelates better to the user\n\n\n\n\n-  Expose the ConnectionPoolStats out the adminAPI\n", "code_len": 233, "nl_len": 202, "path": "client/src/main/java/com/metamatrix/admin/objects/MMConnectionPool.java"}, {"prediction": 0.9566609859466553, "code": "\tprivate void setStats(ConnectionPool connpool, ConnectionPoolStats stats) {\n\n\t\tstats.setConnectionsWaiting(connpool.getNumberOfConnectinsWaiting());\n\t\tstats.setConnectionsCreated(connpool.getTotalCreatedConnectionCount());\n\t\tstats.setConnectionsDestroyed(connpool.getTotalDestroyedConnectionCount());\n\t\tstats.setConnectionsInUse(connpool.getNumberOfConnectionsInUse());\n\t\tstats.setTotalConnections(connpool.getTotalConnectionCount());\n\t}", "nl_input": "Add ability to monitor Connector Connection Pool \nHere are the discussed changes to enable connection pool monitoring:\n\n-   Create a new stat's class to expose the information: ConnectionPoolStats\n-   Enable the ConnectionPool to monitor the following pieces of information:\n\n\na.   Total Connections  - Total Number of Connections for the Connection Pool\nb   Available Connections  - Number of available connections in the connection pool.\nc   Active Connections - Number of Connections currently supporting clients.\nd   Connections Created - Number of Connections created since the Connection Pool was created.\ne  Connections Destroyed  -     Number of Connections destroyed since the Connection Pool was created. \n\nIn the config.xml for the base Connector component type\n\na.  Rename the ConnectorMaxThreads to MaxConnections so that it\nrelates better to the user\n\n\n\n\n-  Expose the ConnectionPoolStats out the adminAPI\n", "code_len": 104, "nl_len": 202, "path": "engine/src/main/java/org/teiid/dqp/internal/pooling/connector/PooledConnector.java"}]}