{"lowest": [{"prediction": 0.09046507626771927, "code": "  /**\n   * Gets the name of the {@link MessageBundle} class.\n   *\n   * @param bundlePath\n   */\n  private String getMessageBundleTypeName(final String bundlePath) {\n    final String typeName =\n        bundlePath.replace(\".json\", \"MessageBundleResource\").replace('/', '.').replace('-', '_').replace('.', '_');\n    return typeName;\n  }", "nl_input": "i18n allow translations in template views to use keys without prefix\nCurrently, data-i18n-prefix is set in templated views, forcing that each translation key to be composed by viewname.key. Problem is that in some cases it can cause keys to be duplicated across different views and translation service. See example below for a common key:\r\n\r\nclose=Close\r\nview1.close=Close\r\nview2.close=Close\r\n\r\nIdeally, we should be able to share the same key (close) across the different views and translationservice.format.\r\n\r\nIt could be that the translation at the template levels would look for prefix at data-i18n-prefix but fallback to no prefix in case that is not found.", "code_len": 84, "nl_len": 162, "path": "errai-ui/src/main/java/org/jboss/errai/ui/rebind/TranslationServiceGenerator.java"}, {"prediction": 0.1119239404797554, "code": "  private static boolean isJsonBundle(final String path) {\n    return path.endsWith(\".json\");\n  }", "nl_input": "i18n allow translations in template views to use keys without prefix\nCurrently, data-i18n-prefix is set in templated views, forcing that each translation key to be composed by viewname.key. Problem is that in some cases it can cause keys to be duplicated across different views and translation service. See example below for a common key:\r\n\r\nclose=Close\r\nview1.close=Close\r\nview2.close=Close\r\n\r\nIdeally, we should be able to share the same key (close) across the different views and translationservice.format.\r\n\r\nIt could be that the translation at the template levels would look for prefix at data-i18n-prefix but fallback to no prefix in case that is not found.", "code_len": 25, "nl_len": 162, "path": "errai-ui/src/main/java/org/jboss/errai/ui/rebind/TranslationServiceGenerator.java"}, {"prediction": 0.12197496742010117, "code": "  /**\n   * Gets the bundle name from the @Bundle annotation.\n   *\n   * @param bundleAnnotatedClass\n   */\n  private String getMessageBundlePath(final MetaClass bundleAnnotatedClass) {\n    final Bundle annotation = bundleAnnotatedClass.getAnnotation(Bundle.class);\n    final String name = annotation.value();\n    if (name == null) {\n      throw new GenerationException(\"@Bundle: bundle name must not be null].\");\n    }\n    // Absolute path vs. relative path.\n    if (name.startsWith(\"/\")) {\n      return name.substring(1);\n    }\n    else {\n      final String packageName = bundleAnnotatedClass.getPackageName();\n      return packageName.replace('.', '/') + \"/\" + name;\n    }\n  }", "nl_input": "i18n allow translations in template views to use keys without prefix\nCurrently, data-i18n-prefix is set in templated views, forcing that each translation key to be composed by viewname.key. Problem is that in some cases it can cause keys to be duplicated across different views and translation service. See example below for a common key:\r\n\r\nclose=Close\r\nview1.close=Close\r\nview2.close=Close\r\n\r\nIdeally, we should be able to share the same key (close) across the different views and translationservice.format.\r\n\r\nIt could be that the translation at the template levels would look for prefix at data-i18n-prefix but fallback to no prefix in case that is not found.", "code_len": 166, "nl_len": 162, "path": "errai-ui/src/main/java/org/jboss/errai/ui/rebind/TranslationServiceGenerator.java"}, {"prediction": 0.12592235207557678, "code": "  /**\n\n   * Registers the bundle with the translation service.\n\n   *\n\n   * @param jsonData\n\n   */\n\n  protected void registerJsonBundle(final String data, final String locale) {\n\n    registerJSON(JSONMap.create(data), locale);\n\n  }", "nl_input": "i18n allow translations in template views to use keys without prefix\nCurrently, data-i18n-prefix is set in templated views, forcing that each translation key to be composed by viewname.key. Problem is that in some cases it can cause keys to be duplicated across different views and translation service. See example below for a common key:\r\n\r\nclose=Close\r\nview1.close=Close\r\nview2.close=Close\r\n\r\nIdeally, we should be able to share the same key (close) across the different views and translationservice.format.\r\n\r\nIt could be that the translation at the template levels would look for prefix at data-i18n-prefix but fallback to no prefix in case that is not found.", "code_len": 67, "nl_len": 162, "path": "errai-ui/src/main/java/org/jboss/errai/ui/client/local/spi/TranslationService.java"}, {"prediction": 0.12799206376075745, "code": "  private static void updateFrameworkProperties(final Map<String, String> frameworkProps, final String key, final String value) {\n    if (frameworkProps.containsKey(key)) {\n      if (isListValuedProperty(key)) {\n        // TODO should validate that different values don't conflict\n        final String oldValue = frameworkProps.get(key);\n        final String newValue = oldValue + \" \" + value;\n        log.debug(\"Merging property {} = {}\", key, newValue);\n        frameworkProps.put(key, newValue);\n      } else {\n        log.warn(\"The property {} has been set multiple times.\", key);\n        frameworkProps.put(key, value);\n      }\n    } else {\n      frameworkProps.put(key, value);\n    }\n  }", "nl_input": "Allow package wildcard usage for filterting marshalling classes\nAllow using package wildcard filtering for including and excluding serializable types.\r\n\r\nEx:\r\nerrai.marshalling.serializableTypes=org.foo.client.* \\\r\n                                    org.abcinc.model.client.*\r\n\r\n\r\nerrai.marshalling.nonserializableTypes=org.foo.client.* \\\r\n                                       org.abcinc.model.client.*", "code_len": 174, "nl_len": 97, "path": "errai-config/src/main/java/org/jboss/errai/config/rebind/EnvUtil.java"}], "q1": [{"prediction": 0.46395760774612427, "code": "    public void write(TProtocol oprot) throws TException {\n      oprot.writeStructBegin(STRUCT_DESC);\n\n      oprot.writeFieldStop();\n      oprot.writeStructEnd();\n    }", "nl_input": "Create a Hive CLI that connects to hive ThriftServer\nWe should have an alternate CLI that works by interacting with the HiveServer, in this way it will be ready when/if we deprecate the current CLI.", "code_len": 44, "nl_len": 46, "path": "service/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/service/ThriftHive.java"}, {"prediction": 0.46397826075553894, "code": "\t@Override\n\tpublic BooleanValue copy() {\n\t\treturn new BooleanValue(this.value);\n\t}", "nl_input": "CopyableValue method to copy into new instance\nProvide a method for generic user-defined functions to clone a {{CopyableValue}}. A common use case is a {{GroupReduceFunction}} that needs to store multiple objects. With object reuse we need to make a deep copy and with type erasure we cannot call new.", "code_len": 26, "nl_len": 65, "path": "flink-core/src/main/java/org/apache/flink/types/BooleanValue.java"}, {"prediction": 0.46413227915763855, "code": "    private String getBaseUrl()\n    {\n        String baseUrl = \"http://\" + host;\n        \n        if( port != null && !\"\".equals( port ) )\n        {\n            baseUrl = baseUrl + \":\" + port;\n        }\n        \n        return baseUrl;\n    }", "nl_input": "add RSS view to repository manager\npossibly needs a new component in JIRA. Items that could have RSS:\n- a particular search\n- preset for \"latest added\" artifacts\n- preset for \"new versions\" of artifacts\n- preset for \"new artifacts from a given sync partner\"", "code_len": 62, "nl_len": 60, "path": "archiva-modules/archiva-web/archiva-rss/src/main/java/org/apache/archiva/rss/processor/NewArtifactsRssFeedProcessor.java"}, {"prediction": 0.46434757113456726, "code": "  /**\n   * @return True if a delete type, a {@link KeyValue.Type#Delete} or\n   * a {KeyValue.Type#DeleteFamily} or a {@link KeyValue.Type#DeleteColumn}\n   * KeyValue type.\n   */\n  public boolean isDelete() {\n    return KeyValue.isDelete(getType());\n  }", "nl_input": "Allow CF to retain deleted rows\nParent allows for a cluster to retain rows for a TTL or keep a minimum number of versions.\nHowever, if a client deletes a row all version older than the delete tomb stone will be remove at the next major compaction (and even at memstore flush - see HBASE-4241).\nThere should be a way to retain those version to guard against software error.\n\nI see two options here:\n1. Add a new flag HColumnDescriptor. Something like \"RETAIN_DELETED\".\n2. Folds this into the parent change. I.e. keep minimum-number-of-versions of versions even past the delete marker.\n\n#1 would allow for more flexibility. #2 comes somewhat naturally with parent (from a user viewpoint)\n\nComments? Any other options?", "code_len": 76, "nl_len": 179, "path": "src/main/java/org/apache/hadoop/hbase/KeyValue.java"}, {"prediction": 0.4644578695297241, "code": "    public void setCar(Car car) {\n        this.car = car;\n    }", "nl_input": "Support remote transaction propagation via SCA binding\nAdd remote transaction propagation on SCA binding so the transaction is propagated to remote SwitchYard node.", "code_len": 21, "nl_len": 29, "path": "demos/transaction-propagation/dealer/src/main/java/org/switchyard/quickstarts/demo/txpropagation/Offer.java"}], "median": [{"prediction": 0.6167894601821899, "code": "  /** Get the attempt finish time */\n  public long getFinishTime() { return datum.finishTime; }", "nl_input": "Modify JobHistory to use Avro for serialization instead of raw JSON\nMAPREDUCE-157 modifies JobHistory to log events using Json Format.  This can be modified to use Avro instead. ", "code_len": 24, "nl_len": 42, "path": "src/java/org/apache/hadoop/mapreduce/jobhistory/TaskAttemptFinishedEvent.java"}, {"prediction": 0.6167894601821899, "code": "  /** Get the attempt finish time */\n  public long getFinishTime() { return datum.finishTime; }", "nl_input": "Modify JobHistory to use Avro for serialization instead of raw JSON\nMAPREDUCE-157 modifies JobHistory to log events using Json Format.  This can be modified to use Avro instead. ", "code_len": 24, "nl_len": 42, "path": "src/java/org/apache/hadoop/mapreduce/jobhistory/MapAttemptFinishedEvent.java"}, {"prediction": 0.6170892715454102, "code": "  private void checkStream() {\n    if (stream == 0) {\n      throw new NullPointerException(\"Stream not initialized\");\n    }\n  }", "nl_input": "Add Codec for ZStandard Compression\nZStandard: https://github.com/facebook/zstd has been used in production for 6 months by facebook now.  v1.0 was recently released.  Create a codec for this library.  ", "code_len": 32, "nl_len": 51, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java"}, {"prediction": 0.6172969937324524, "code": "  @Override\n  public void doAs(UserGroupInformation ugi, PrivilegedExceptionAction<Void> pvea) throws\n    IOException, InterruptedException {\n    try {\n      Subject.doAs(SecurityUtil.getSubject(ugi),pvea);\n    } catch (PrivilegedActionException e) {\n      throw new IOException(e);\n    }\n  }", "nl_input": "Passing user identity from metastore client to server in non-secure mode\nCurrently in unsecure mode client don't pass on user identity. As a result hdfs and other operations done by server gets executed by user running metastore process instead of being done in context of client. This results in problem as reported here: \nhttp://mail-archives.apache.org/mod_mbox/hive-user/201111.mbox/%3CCAK0mCrRC3aPqtRHDe2J25Rm0JX6TS1KXxd7KPjqJjoqBjg=APA@mail.gmail.com%3E", "code_len": 81, "nl_len": 137, "path": "shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java"}, {"prediction": 0.6173900961875916, "code": "  /**\n   * start a new session and set it to current session.\n   */\n  public static SessionState start(HiveConf conf) {\n    SessionState ss = new SessionState(conf);\n    return start(ss);\n  }", "nl_input": "Create a Hive CLI that connects to hive ThriftServer\nWe should have an alternate CLI that works by interacting with the HiveServer, in this way it will be ready when/if we deprecate the current CLI.", "code_len": 54, "nl_len": 46, "path": "ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java"}], "q3": [{"prediction": 0.7581362724304199, "code": "    private void processNewVersionsOfArtifact( List<ArchivaArtifact> data )\n    {\n        String repoId = getRepoId( data );\n\n        List<String> artifacts = new ArrayList<String>();\n\n        for ( ArchivaArtifact artifact : data )\n        {\n            artifacts.add( artifact.toString() );\n        }\n\n        Collections.sort( artifacts );\n\n        Map<String, String> artifactsMap = toMap( artifacts );\n\n        for ( String key : artifactsMap.keySet() )\n        {\n            List<RssFeedEntry> entries = new ArrayList<RssFeedEntry>();\n            String artifactPath = getArtifactPath( key );\n            RssFeedEntry entry =\n                new RssFeedEntry( NEW_VERSIONS_OF_ARTIFACT + \"\\'\" + key + \"\\'\" + \" as of \" +\n                    Calendar.getInstance().getTime(), \"http://localhost:8080/archiva/repository/\" + repoId + \"/\" +\n                    artifactPath );\n\n            String description =\n                \"These are the new versions of artifact \" + \"\\'\" + key + \"\\'\" + \" in the repository: \\n\" +\n                    StringUtils.replace( ( (String) artifactsMap.get( key ) ), \"|\", \"\\n\" );\n\n            entry.setDescription( description );\n            entries.add( entry );\n\n            generateFeed( \"new_versions_\" + repoId + \"_\" + key + \".xml\", NEW_VERSIONS_OF_ARTIFACT + \"\\'\" + key + \"\\'\",\n                          \"http://localhost:8080/archiva/repository/\" + repoId + \"/\" + artifactPath,\n                          \"New versions of artifact \" + \"\\'\" + key + \"\\' found in repository \" + \"\\'\" + repoId + \"\\'\" +\n                              \" during repository scan.\", entries );\n        }\n    }", "nl_input": "add RSS view to repository manager\npossibly needs a new component in JIRA. Items that could have RSS:\n- a particular search\n- preset for \"latest added\" artifacts\n- preset for \"new versions\" of artifacts\n- preset for \"new artifacts from a given sync partner\"", "code_len": 406, "nl_len": 60, "path": "archiva-modules/archiva-web/archiva-rss/src/main/java/org/apache/archiva/rss/processor/NewArtifactsRssFeedProcessor.java"}, {"prediction": 0.758167564868927, "code": "    public void visit( Create obj ) {\n        append(CREATE);\n        append(SPACE);\n        append(LOCAL);\n        append(SPACE);\n        append(TEMPORARY);\n        append(SPACE);\n        append(TABLE);\n        append(SPACE);\n        visitNode(obj.getTable());\n        append(SPACE);\n\n        // Columns clause\n        List<Column> columns = obj.getColumns();\n        append(\"(\"); //$NON-NLS-1$\n        Iterator<Column> iter = columns.iterator();\n        while (iter.hasNext()) {\n            Column element = iter.next();\n            outputDisplayName(element.getName());\n            append(SPACE);\n            if (element.isAutoIncremented()) {\n            \tappend(NonReserved.SERIAL);\n            } else {\n\t            append(element.getRuntimeType());\n\t            if (element.getNullType() == NullType.No_Nulls) {\n\t            \tappend(NOT);\n\t            \tappend(SPACE);\n\t            \tappend(NULL);\n\t            }\n            }\n            if (iter.hasNext()) {\n                append(\", \"); //$NON-NLS-1$\n            }\n        }\n        if (!obj.getPrimaryKey().isEmpty()) {\n            append(\", \"); //$NON-NLS-1$\n            append(PRIMARY);\n            append(\" \"); //$NON-NLS-1$\n            append(NonReserved.KEY);\n            append(Tokens.LPAREN);\n            Iterator<ElementSymbol> pkiter = obj.getPrimaryKey().iterator();\n            while (pkiter.hasNext()) {\n                outputShortName(pkiter.next());\n                if (pkiter.hasNext()) {\n                    append(\", \"); //$NON-NLS-1$\n                }\n            }\n            append(Tokens.RPAREN);\n        }\n        append(\")\"); //$NON-NLS-1$\n    }", "nl_input": "Add support for not null and auto increment for temp table columns\nIt would be good to have auto increment and not null support for temp tables.", "code_len": 413, "nl_len": 29, "path": "engine/src/main/java/org/teiid/query/sql/visitor/SQLStringVisitor.java"}, {"prediction": 0.7582709789276123, "code": "  @Override\n  public Iterator<LongStatistic> getLongStatistics() {\n    return new LongStatisticIterator(stats.getData());\n  }", "nl_input": "Add a new interface for retrieving FS and FC Statistics\nCurrently FileSystem.Statistics exposes the following statistics:\nBytesRead\nBytesWritten\nReadOps\nLargeReadOps\nWriteOps\n\nThese are in-turn exposed as job counters by MapReduce and other frameworks. There is logic within DfsClient to map operations to these counters that can be confusing, for instance, mkdirs counts as a writeOp.\n\nProposed enhancement:\nAdd a statistic for each DfsClient operation including create, append, createSymlink, delete, exists, mkdirs, rename and expose them as new properties on the Statistics object. The operation-specific counters can be used for analyzing the load imposed by a particular job on HDFS. \nFor example, we can use them to identify jobs that end up creating a large number of files.\n\nOnce this information is available in the Statistics object, the app frameworks like MapReduce can expose them as additional counters to be aggregated and recorded as part of job summary.", "code_len": 31, "nl_len": 211, "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java"}, {"prediction": 0.7583358287811279, "code": "    public String getName() {\n        return _name;\n    }", "nl_input": "Support RESTEasy as a provider for REST gateway bindings\nCreate a distinct REST Gateway component based on RESTEasy.  User should be able to specify an annotated JAX-RS class for the endpoint configuration.  Ideally, the endpoint is registered directly with the existing AS http listener instead of creating a distinct HTTP listener for this gateway.", "code_len": 15, "nl_len": 69, "path": "rest-binding/src/main/java/org/switchyard/quickstarts/rest/binding/Item.java"}, {"prediction": 0.7584187984466553, "code": "    /**\n     * Stop lifecycle.\n     */\n    public void stop() {\n        _endpoint.stop();\n    }", "nl_input": "Basic HTTP Gateway\nCreate support for a basic HTTP gateway.", "code_len": 28, "nl_len": 12, "path": "http/src/main/java/org/switchyard/component/http/InboundHandler.java"}], "highest": [{"prediction": 0.9421494007110596, "code": "\t@Override\n\tprotected String getSequenceQuery() {\n\t    return \"select null as sequence_catalog, sequence_owner as sequence_schema, sequence_name from (select sequence_name, sequence_owner from all_sequences union select synonym_name, table_owner from all_synonyms us, all_sequences asq where asq.sequence_name = us.table_name and asq.sequence_owner = us.table_owner) \" //$NON-NLS-1$\n\t            + \"where sequence_owner like ? and sequence_name like ?\"; //$NON-NLS-1$\n\t}", "nl_input": "Document/expand sequence support\nCurrently sequence workaround logic only exists for oracle and is undocumented.  We should look at expanding sequence support - even for dynamic vdbs, see SQuriel's handling of system queries for retrieving sequence metadata.\n\nAt least allowing the workaround logic to work for all sources that support sequences (Postgres, DB2, etc.) would be good.", "code_len": 132, "nl_len": 80, "path": "connectors/jdbc/translator-jdbc/src/main/java/org/teiid/translator/jdbc/oracle/OracleMetadataProcessor.java"}, {"prediction": 0.9440761208534241, "code": "    /**\n     * Override method to utilize the cache\n     */\n    protected Metadata getMetadata( Artifact artifact )\n        throws RepositoryQueryLayerException\n    {\n        Metadata metadata = null;\n        \n        if ( cacheMetadata.containsKey( artifact.getId() ) )\n        {\n            cacheHits++;\n            metadata = (Metadata) cacheMetadata.get( artifact.getId() );\n        }\n        else\n        {\n            metadata = super.getMetadata( artifact );\n            cacheMetadata.put( artifact.getId(), metadata );\n        }\n        \n        return metadata;\n    }", "nl_input": "caching repository query interface\nneed to be able to query the repository to see if artifacts exist, what versions are available, etc. This needs to interoperate with the indexing, and where applicable should cache information, balancing the need to keep a low memory overhead but avoid repetitive disk reads on metadata files.", "code_len": 127, "nl_len": 65, "path": "maven-repository-reports-standard/src/main/java/org/apache/maven/repository/reporting/CachedRepositoryQueryLayer.java"}, {"prediction": 0.9501513838768005, "code": "    /**\n     * Override method to utilize the cache\n     */\n    protected Metadata getMetadata( Artifact artifact )\n        throws RepositoryQueryLayerException\n    {\n        Metadata metadata = (Metadata) cache.get( artifact.getId() );\n        \n        if ( metadata == null )\n        {\n            metadata = super.getMetadata( artifact );\n            cache.put( artifact.getId(), metadata );\n        }\n        \n        return metadata;\n    }", "nl_input": "caching repository query interface\nneed to be able to query the repository to see if artifacts exist, what versions are available, etc. This needs to interoperate with the indexing, and where applicable should cache information, balancing the need to keep a low memory overhead but avoid repetitive disk reads on metadata files.", "code_len": 98, "nl_len": 65, "path": "maven-repository-reports-standard/src/main/java/org/apache/maven/repository/reporting/CachedRepositoryQueryLayer.java"}, {"prediction": 0.9561300277709961, "code": "\tprivate MMConnectionPool getStats(ConnectionPool connpool) {\n\t\tMMConnectionPool stats = new MMConnectionPool();\n\t\tstats.setConnectionsWaiting(connpool.getNumberOfConnectinsWaiting());\n\t\tstats.setConnectionsCreated(connpool.getTotalCreatedConnectionCount());\n\t\tstats.setConnectionsDestroyed(connpool.getTotalDestroyedConnectionCount());\n\t\tstats.setConnectionsInUse(connpool.getNumberOfConnectionsInUse());\n\t\tstats.setTotalConnections(connpool.getTotalConnectionCount());\n\t\treturn stats;\n\t}", "nl_input": "Add ability to monitor Connector Connection Pool \nHere are the discussed changes to enable connection pool monitoring:\n\n-   Create a new stat's class to expose the information: ConnectionPoolStats\n-   Enable the ConnectionPool to monitor the following pieces of information:\n\n\na.   Total Connections  - Total Number of Connections for the Connection Pool\nb   Available Connections  - Number of available connections in the connection pool.\nc   Active Connections - Number of Connections currently supporting clients.\nd   Connections Created - Number of Connections created since the Connection Pool was created.\ne  Connections Destroyed  -     Number of Connections destroyed since the Connection Pool was created. \n\nIn the config.xml for the base Connector component type\n\na.  Rename the ConnectorMaxThreads to MaxConnections so that it\nrelates better to the user\n\n\n\n\n-  Expose the ConnectionPoolStats out the adminAPI\n", "code_len": 119, "nl_len": 202, "path": "engine/src/main/java/org/teiid/dqp/internal/pooling/connector/PooledConnector.java"}, {"prediction": 0.95663982629776, "code": "  static void redirectToRandomDataNode(final NameNode nn, \n                                       HttpServletRequest request,\n                                       HttpServletResponse resp,\n                                       Configuration conf\n                                       ) throws IOException,\n                                                InterruptedException {\n    final DatanodeID datanode = nn.getNamesystem().getRandomDatanode();\n    final String user = request.getRemoteUser();\n    String tokenString = getDelegationToken(nn, user);\n    // if the user is defined, get a delegation token and stringify it\n    final String redirectLocation;\n    final String nodeToRedirect;\n    int redirectPort;\n    if (datanode != null) {\n      nodeToRedirect = datanode.getHost();\n      redirectPort = datanode.getInfoPort();\n    } else {\n      nodeToRedirect = nn.getHttpAddress().getHostName();\n      redirectPort = nn.getHttpAddress().getPort();\n    }\n    String fqdn = InetAddress.getByName(nodeToRedirect).getCanonicalHostName();\n    redirectLocation = \"http://\" + fqdn + \":\" + redirectPort\n        + \"/browseDirectory.jsp?namenodeInfoPort=\"\n        + nn.getHttpAddress().getPort() + \"&dir=/\"\n        + (tokenString == null ? \"\" :\n           JspHelper.SET_DELEGATION + tokenString);\n    resp.sendRedirect(redirectLocation);\n  }", "nl_input": "Allow browsing the filesystem over http using delegation tokens\nAssuming the user authenticates to the NameNode in the browser, allow them to browse the file system by adding a delegation token the the url when it is redirected to a datanode.", "code_len": 295, "nl_len": 53, "path": "src/java/org/apache/hadoop/hdfs/server/namenode/NamenodeJspHelper.java"}]}