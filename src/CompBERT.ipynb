{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqY3lDjoFbeA"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sdbHJGvB3USY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c99a88-ba0d-4eb5-b4f6-0729281e54e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-RlRHHTTFgdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8904516-7e40-4726-f224-671ab1e65faa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Master Thesis/compbert\n",
            "\u001b[0m\u001b[01;34mdataset\u001b[0m/  requirements.txt  \u001b[01;34msaved_models\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%cd \"drive/MyDrive/Master Thesis/compbert\"\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "VZOTF6aqFUln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2f9fdd7-3000-4dfd-b0e1-dd372df78627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torcheval\n",
            "Successfully installed torcheval-0.0.7\n"
          ]
        }
      ],
      "source": [
        "! pip install torch transformers numpy torcheval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_-Zyyz_F7bb"
      },
      "source": [
        "# Imports and arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Al8TQjihF6BZ"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "from pathlib import Path\n",
        "from importlib import reload\n",
        "from torcheval.metrics.functional import binary_auprc, binary_auroc\n",
        "from torch.utils.data import (\n",
        "    Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        ")\n",
        "from transformers import (\n",
        "    RobertaModel, RobertaTokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0MKvNchqJYkS"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.output_dir = (\n",
        "            \"/content/drive/MyDrive/Master Thesis/compbert/saved_models\"\n",
        "        )\n",
        "        self.model_name_or_path = \"microsoft/unixcoder-base\"\n",
        "        self.do_train = False\n",
        "        self.do_eval = True\n",
        "        self.train_data_file = (\n",
        "            \"/content/drive/MyDrive/Master Thesis/compbert/dataset/\"\n",
        "            \"data_formatted2_sample.json\"\n",
        "        )\n",
        "        self.eval_data_file = (\n",
        "            \"/content/drive/MyDrive/Master Thesis/compbert/dataset/\"\n",
        "            \"data_formatted2_sample.json\"\n",
        "        )\n",
        "        self.dev_data_file = (\n",
        "            \"/content/drive/MyDrive/Master Thesis/compbert/dataset/\"\n",
        "            \"data_formatted2_sample.json\"\n",
        "        )\n",
        "        self.balance_type = \"full_set_balanced\"\n",
        "        self.num_train_epochs = 2\n",
        "        self.code_length = 512\n",
        "        self.nl_length = 256\n",
        "        self.train_batch_size = 24\n",
        "        self.eval_batch_size = 24\n",
        "        self.learning_rate = 1e-5\n",
        "        self.max_grad_norm = 1.0\n",
        "        self.seed = 123456\n",
        "\n",
        "        self.device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "        self.n_gpu = torch.cuda.device_count()\n",
        "\n",
        "args = Args()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FLTo9MtIiaG"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KR6x6m8uIj_R"
      },
      "outputs": [],
      "source": [
        "random.seed(args.seed)\n",
        "os.environ['PYHTONHASHSEED'] = str(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Set log\n",
        "reload(logging)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4RZ0EidF-jc"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mXQXR6JzF98V"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "    A neural network model that encodes code and natural language (NL) inputs\n",
        "    using a shared encoder and normalizes the output representations. This\n",
        "    model can handle either code or NL inputs, depending on which is provided.\n",
        "\n",
        "    Attributes:\n",
        "        encoder (nn.Module): The encoder module used to generate embeddings\n",
        "                             for the input data.\n",
        "\n",
        "    Methods:\n",
        "        forward(code_inputs=None, nl_inputs=None):\n",
        "            Performs a forward pass through the encoder using either code or NL\n",
        "            inputs and returns the normalized output embeddings. If\n",
        "            `code_inputs` is provided, it processes the code inputs; otherwise,\n",
        "            it processes the NL inputs.\n",
        "\n",
        "            Args:\n",
        "                code_inputs (torch.Tensor, optional): Tokenized code input\n",
        "                                                      tensor. Defaults to None.\n",
        "                nl_inputs (torch.Tensor, optional): Tokenized NL input tensor.\n",
        "                                                    Defaults to None.\n",
        "\n",
        "            Returns:\n",
        "                torch.Tensor: Normalized output embeddings of the input data.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = encoder\n",
        "\n",
        "    def forward(self, code_inputs=None, nl_inputs=None):\n",
        "        if code_inputs is not None:\n",
        "            outputs = self.encoder(\n",
        "                code_inputs, attention_mask=code_inputs.ne(1)\n",
        "            )[0]\n",
        "            outputs = (\n",
        "                (outputs * code_inputs.ne(1)[:, :, None]).sum(1)\n",
        "                / code_inputs.ne(1).sum(-1)[:, None]\n",
        "            )\n",
        "            return torch.nn.functional.normalize(outputs, p=2, dim=1)\n",
        "        else:\n",
        "            outputs = self.encoder(\n",
        "                nl_inputs, attention_mask=nl_inputs.ne(1)\n",
        "            )[0]\n",
        "            outputs = (\n",
        "                (outputs * nl_inputs.ne(1)[:, :, None]).sum(1)\n",
        "                / nl_inputs.ne(1).sum(-1)[:, None]\n",
        "            )\n",
        "            return torch.nn.functional.normalize(outputs, p=2, dim=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcxAqZk9HRkj"
      },
      "source": [
        "# Data classes and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kSeLAqAxHZIC"
      },
      "outputs": [],
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"\n",
        "    A class to represent a single set of features for training or testing.\n",
        "\n",
        "    This class encapsulates the features extracted from an example, including\n",
        "    tokenized inputs and metadata. It is used to store and pass around the\n",
        "    processed data for a single example, which includes both code and natural\n",
        "    language inputs.\n",
        "\n",
        "    Attributes:\n",
        "        code_tokens (list of str): List of tokenized code tokens.\n",
        "        code_ids (list of int): List of token IDs corresponding to the code\n",
        "                                tokens.\n",
        "        nl_tokens (list of str): List of tokenized natural language tokens.\n",
        "        nl_ids (list of int): List of token IDs corresponding to the natural\n",
        "                              language tokens.\n",
        "        changed (bool): Indicator of whether the code has been modified.\n",
        "        idx (int): Index of the example in the dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 code_tokens,\n",
        "                 code_ids,\n",
        "                 nl_tokens,\n",
        "                 nl_ids,\n",
        "                 changed,\n",
        "                 idx):\n",
        "        self.code_tokens = code_tokens\n",
        "        self.code_ids = code_ids\n",
        "        self.nl_tokens = nl_tokens\n",
        "        self.nl_ids = nl_ids\n",
        "        self.changed = changed\n",
        "        self.idx = idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O7OpIg6jHdfH"
      },
      "outputs": [],
      "source": [
        "def convert_examples_to_features(entry, idx, tokenizer):\n",
        "    \"\"\"\n",
        "    Convert a single example into token IDs suitable for model input.\n",
        "\n",
        "    This function tokenizes the code and natural language input from the\n",
        "    provided example, truncates or pads them to the specified lengths, and\n",
        "    then converts these tokens into token IDs using the provided tokenizer.\n",
        "\n",
        "    Args:\n",
        "        entry (dict): A dictionary containing 'code', 'nl_input', and 'changed'\n",
        "                      keys. 'code' and 'nl_input' are the texts to be tokenized,\n",
        "                      and 'changed' indicates if the code has been modified.\n",
        "        idx (int): The index of the example in the dataset.\n",
        "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer instance\n",
        "                  used to convert text to token IDs.\n",
        "\n",
        "    Returns:\n",
        "        InputFeatures: An instance of InputFeatures containing tokenized and\n",
        "                       padded code and natural language inputs, along with\n",
        "                       additional metadata (e.g., the 'changed' flag and index).\n",
        "                       Returns None if the tokenized inputs exceed the maximum\n",
        "                       allowed length.\n",
        "    \"\"\"\n",
        "    code_tokens = tokenizer.tokenize(entry[\"code\"])\n",
        "    nl_tokens = tokenizer.tokenize(entry[\"nl_input\"])\n",
        "\n",
        "    # print(f\"code: {len(code_tokens)} - nl: {len(nl_tokens)}\")\n",
        "    if (len(code_tokens) > args.code_length - 4\n",
        "            or len(nl_tokens) > args.nl_length - 4):\n",
        "        return None\n",
        "\n",
        "    code_tokens = [\n",
        "        tokenizer.cls_token, \"<encoder-only>\", tokenizer.sep_token\n",
        "    ] + code_tokens + [tokenizer.sep_token]\n",
        "    code_ids = tokenizer.convert_tokens_to_ids(code_tokens)\n",
        "    padding_length = args.code_length - len(code_ids)\n",
        "    code_ids += [tokenizer.pad_token_id] * padding_length\n",
        "\n",
        "    nl_tokens = [\n",
        "        tokenizer.cls_token, \"<encoder-only>\", tokenizer.sep_token\n",
        "    ] + nl_tokens + [tokenizer.sep_token]\n",
        "    nl_ids = tokenizer.convert_tokens_to_ids(nl_tokens)\n",
        "    padding_length = args.nl_length - len(nl_ids)\n",
        "    nl_ids += [tokenizer.pad_token_id] * padding_length\n",
        "\n",
        "    return InputFeatures(\n",
        "        code_tokens, code_ids, nl_tokens, nl_ids, entry[\"changed\"], idx\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5CUoXBGqHk7o"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A dataset class for handling text examples, including code and natural\n",
        "    language inputs. This class is responsible for loading, processing, and\n",
        "    balancing examples from a JSON file according to the specified balance type.\n",
        "\n",
        "    Attributes:\n",
        "        examples (list): A list of `InputFeatures` objects representing\n",
        "                         the processed examples.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, file_path=None,\n",
        "                 balance_type=\"full_set_imbalanced\", show_example=False):\n",
        "        \"\"\"\n",
        "        Initializes the dataset by loading examples from a JSON file,\n",
        "        processing them, and balancing them according to the specified\n",
        "        balance type.\n",
        "\n",
        "        Args:\n",
        "            tokenizer (PreTrainedTokenizer): The tokenizer to use for\n",
        "                                             tokenizing the code and\n",
        "                                             natural language inputs.\n",
        "            file_path (str, optional): Path to the JSON file containing the\n",
        "                                       dataset. Defaults to None.\n",
        "            balance_type (str, optional): Specifies how to balance the dataset.\n",
        "                                           Options are \"full_set_balanced\",\n",
        "                                           \"full_set_imbalanced\",\n",
        "                                           \"one_per_issue\", and\n",
        "                                           \"multiple_per_issue\". Defaults\n",
        "                                           to \"full_set_imbalanced\".\n",
        "            show_example (bool, optional): If True, prints a few examples\n",
        "                                            from the dataset for inspection.\n",
        "                                            Defaults to False.\n",
        "        \"\"\"\n",
        "        self.examples = []\n",
        "\n",
        "        file_path = Path(file_path)\n",
        "        with file_path.open(\"r\") as f:\n",
        "            entries = json.load(f)\n",
        "\n",
        "        logger.info(f\"Dataset size at start: {len(entries)}\")\n",
        "\n",
        "        large_examples = 0\n",
        "        issue = \"\"\n",
        "        pos_examples, neg_examples = [], []\n",
        "        last_idx = len(entries) - 1\n",
        "        for idx, entry in enumerate(entries):\n",
        "            if (balance_type in {\"one_per_issue\", \"multiple_per_issue\"}\n",
        "                and (issue != entry[\"issue\"] or idx == last_idx)):\n",
        "\n",
        "                if balance_type == \"one_per_issue\":\n",
        "                    if (len(pos_examples) != 0) and (len(neg_examples) != 0):\n",
        "                        pos_example = random.choice(pos_examples)\n",
        "                        neg_example = random.choice(neg_examples)\n",
        "                        self.examples += [pos_example, neg_example]\n",
        "\n",
        "                elif balance_type == \"multiple_per_issue\":\n",
        "                    self.examples += balance_multiple(\n",
        "                        pos_examples, neg_examples\n",
        "                    )\n",
        "\n",
        "                pos_examples, neg_examples = [], []\n",
        "                issue = entry[\"issue\"]\n",
        "\n",
        "            feature = convert_examples_to_features(entry, idx, tokenizer)\n",
        "            if feature is None:\n",
        "                large_examples += 1\n",
        "            elif entry[\"changed\"]:\n",
        "                pos_examples.append(feature)\n",
        "            else:\n",
        "                neg_examples.append(feature)\n",
        "\n",
        "        if balance_type == \"full_set_balanced\":\n",
        "            self.examples += balance_multiple(pos_examples, neg_examples)\n",
        "\n",
        "        elif balance_type == \"full_set_imbalanced\":\n",
        "            logger.info(f\"  n positive examples = {len(pos_examples)}\")\n",
        "            logger.info(f\"  n negative examples = {len(neg_examples)}\")\n",
        "            self.examples += pos_examples + neg_examples\n",
        "\n",
        "        logger.info(f\"Final dataset size: {len(self.examples)}\")\n",
        "        logger.info(\n",
        "            f\"Removed entries due to exceeding token limit: {large_examples}\"\n",
        "        )\n",
        "\n",
        "        if show_example:\n",
        "            for idx, example in enumerate(self.examples[:3]):\n",
        "                logger.info(\"*** Example ***\")\n",
        "                logger.info(f\"idx: {idx}\")\n",
        "                logger.info(\"code_tokens: {}\".format(\n",
        "                    [x.replace('\\u0120', '_') for x in example.code_tokens]))\n",
        "                logger.info(f\"code_ids: {' '.join(map(str, example.code_ids))}\")\n",
        "                logger.info(\"nl_tokens: {}\".format(\n",
        "                    [x.replace('\\u0120', '_') for x in example.nl_tokens]))\n",
        "                logger.info(f\"nl_ids: {' '.join(map(str, example.nl_ids))}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of examples in the dataset.\"\"\"\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves the example at the specified index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): The index of the example to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the token IDs and metadata for the\n",
        "                  example.\n",
        "        \"\"\"\n",
        "        example = self.examples[idx]\n",
        "        return {\"code_input\": torch.tensor(example.code_ids),\n",
        "                \"nl_input\": torch.tensor(example.nl_ids),\n",
        "                \"changed\": torch.tensor(example.changed),\n",
        "                \"idx\": example.idx}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Mm34gY3eenfW"
      },
      "outputs": [],
      "source": [
        "def balance_multiple(pos_examples, neg_examples):\n",
        "    \"\"\"\n",
        "    Balances two lists of examples by truncating the larger list to match\n",
        "    the size of the smaller list. The lists are shuffled before truncation\n",
        "    to ensure randomness in the selection.\n",
        "\n",
        "    Args:\n",
        "        pos_examples (list): A list of positive examples.\n",
        "        neg_examples (list): A list of negative examples.\n",
        "\n",
        "    Returns:\n",
        "        list: A combined list of positive and negative examples, balanced\n",
        "              to have the same number of examples from each list. The\n",
        "              examples are shuffled before truncation to ensure randomness.\n",
        "    \"\"\"\n",
        "    len_positives = len(pos_examples)\n",
        "    len_negatives = len(neg_examples)\n",
        "    if len_positives < len_negatives:\n",
        "        np.random.shuffle(neg_examples)\n",
        "        neg_examples = neg_examples[:len_positives]\n",
        "    elif len_positives > len_negatives:\n",
        "        np.random.shuffle(pos_examples)\n",
        "        pos_examples = pos_examples[:len_negatives]\n",
        "\n",
        "    assert len(pos_examples) == len(neg_examples)\n",
        "    return pos_examples + neg_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZNUzYlEKbgU"
      },
      "source": [
        "# Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "d-xW3CyDKgve"
      },
      "outputs": [],
      "source": [
        "def train(model, tokenizer):\n",
        "    \"\"\"\n",
        "    Trains the model using the specified tokenizer.\n",
        "\n",
        "    This function performs the training of the given model on the dataset\n",
        "    provided by `args.train_data_file`. It handles the creation of training\n",
        "    and evaluation datasets, sets up the optimizer and learning rate scheduler,\n",
        "    and manages the training loop. It also logs various training metrics and\n",
        "    saves model checkpoints based on evaluation performance.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to be trained.\n",
        "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer used to\n",
        "                                                      preprocess the text data.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Get training dataset\n",
        "    train_dataset = TextDataset(\n",
        "        tokenizer,\n",
        "        args.train_data_file,\n",
        "        balance_type=args.balance_type,\n",
        "        show_example=True\n",
        "    )\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        sampler=train_sampler,\n",
        "        batch_size=args.train_batch_size,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    # Get evaluation dataset\n",
        "    dev_dataset = TextDataset(\n",
        "        tokenizer,\n",
        "        args.dev_data_file,\n",
        "        balance_type=\"full_set_imbalanced\"\n",
        "    )\n",
        "\n",
        "    # Get optimizer and scheduler\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=args.learning_rate,\n",
        "        eps=1e-8\n",
        "    )\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=len(train_dataloader) * args.num_train_epochs\n",
        "    )\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
        "    logger.info(\n",
        "        \"  Instantaneous batch size per GPU = \"\n",
        "        f\"{args.train_batch_size // args.n_gpu}\"\n",
        "    )\n",
        "    logger.info(\n",
        "        f\"  Total train batch size = {args.train_batch_size}\"\n",
        "    )\n",
        "    logger.info(\n",
        "        \"  Total optimization steps = \"\n",
        "        f\"{len(train_dataloader) * args.num_train_epochs}\"\n",
        "    )\n",
        "\n",
        "    evaluation_interval = ceil(len(train_dataloader) / 5)\n",
        "    loss_progress = []\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    checkpoint_prefix = \"checkpoint-best\"\n",
        "    output_dir_path = Path(args.output_dir).joinpath(checkpoint_prefix)\n",
        "    if not os.path.exists(output_dir_path):\n",
        "        os.makedirs(output_dir_path)\n",
        "\n",
        "    model.train()\n",
        "    tr_num, tr_loss, best_score = 0, 0, 0\n",
        "    for idx in range(args.num_train_epochs):\n",
        "        n_interval = 0\n",
        "\n",
        "        if idx != 0:\n",
        "            # Reset sampling\n",
        "            train_dataset = TextDataset(\n",
        "                tokenizer,\n",
        "                args.train_data_file,\n",
        "                balance_type=args.balance_type\n",
        "            )\n",
        "            train_sampler = RandomSampler(train_dataset)\n",
        "            train_dataloader = DataLoader(\n",
        "                train_dataset,\n",
        "                sampler=train_sampler,\n",
        "                batch_size=args.train_batch_size,\n",
        "                num_workers=4\n",
        "            )\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            # Get inputs\n",
        "            code_inputs = batch[\"code_input\"].to(args.device)\n",
        "            nl_inputs = batch[\"nl_input\"].to(args.device)\n",
        "            labels = batch[\"changed\"].to(args.device).float()\n",
        "\n",
        "            code_vecs = model(code_inputs=code_inputs)\n",
        "            nl_vecs = model(nl_inputs=nl_inputs)\n",
        "\n",
        "            # Calculate scores and loss\n",
        "            scores = torch.einsum(\"ij,ij->i\", code_vecs, nl_vecs)\n",
        "            loss_fct = nn.BCEWithLogitsLoss()\n",
        "            loss = loss_fct(scores * 5, labels)\n",
        "\n",
        "            # Report loss\n",
        "            tr_loss += loss.item()\n",
        "            tr_num += 1\n",
        "            if (step + 1) % 100 == 0:\n",
        "                loss_report = round(tr_loss / tr_num, 5)\n",
        "                logger.info(f\"epoch {idx} step {step+1} loss {loss_report}\")\n",
        "                tr_loss = 0\n",
        "                tr_num = 0\n",
        "                loss_progress.append((idx, step + 1, loss_report))\n",
        "\n",
        "            # Backward\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                model.parameters(), args.max_grad_norm\n",
        "            )\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            scheduler.step()\n",
        "\n",
        "            if (step + 1) % evaluation_interval == 0:\n",
        "                best_score = create_checkpoint(\n",
        "                    model,\n",
        "                    tokenizer,\n",
        "                    dev_dataset,\n",
        "                    best_score,\n",
        "                    output_dir_path,\n",
        "                    loss_progress,\n",
        "                    idx,\n",
        "                    n_interval\n",
        "                )\n",
        "                n_interval += 1\n",
        "\n",
        "        best_score = create_checkpoint(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            dev_dataset,\n",
        "            best_score,\n",
        "            output_dir_path,\n",
        "            loss_progress,\n",
        "            idx,\n",
        "            n_interval\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IpPFXh4zeywR"
      },
      "outputs": [],
      "source": [
        "def create_checkpoint(\n",
        "    model, tokenizer, dev_dataset, best_score, output_dir_path,\n",
        "    loss_progress, idx, n_interval\n",
        "):\n",
        "    \"\"\"\n",
        "    Saves the checkpoint if the model performance has improved.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to be evaluated and saved.\n",
        "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer used\n",
        "                                                      for processing text data.\n",
        "        dev_dataset (TextDataset): The dataset used for evaluation.\n",
        "        best_score (float): The current best score for comparison.\n",
        "        output_dir_path (Path): Directory path to save the checkpoint and\n",
        "                                evaluation results.\n",
        "        loss_progress (list): List of loss values recorded during training.\n",
        "        idx (int): The current epoch index.\n",
        "        n_interval (int): The current interval index for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        float: The updated best score after evaluation.\n",
        "    \"\"\"\n",
        "    results = evaluate(model, tokenizer, dev_dataset)\n",
        "    for key, value in results.items():\n",
        "        if key not in {\"predictions\", \"predictions_mult\", \"labels\", \"idxs\"}:\n",
        "            logger.info(\"  %s = %s\", key, round(value, 4))\n",
        "\n",
        "    #save best model\n",
        "    auprc = results[\"auprc\"]\n",
        "    if auprc > best_score:\n",
        "        best_score = auprc\n",
        "        logger.info(\"  \"+\"*\" * 20)\n",
        "        logger.info(f\"  Best auprc: {round(best_score, 4)}\")\n",
        "        logger.info(\"  \"+\"*\" * 20)\n",
        "\n",
        "        model_to_save = model.module if hasattr(model, \"module\") else model\n",
        "        output_path = output_dir_path.joinpath(f\"model.bin\")\n",
        "        torch.save(model_to_save.state_dict(), output_path)\n",
        "        logger.info(\"Saving model checkpoint to %s\", output_path)\n",
        "\n",
        "    results[\"loss_progress\"] = loss_progress\n",
        "    results_path = output_dir_path.joinpath(\n",
        "        f\"train_eval_{idx}_{n_interval}.json\"\n",
        "    )\n",
        "    with results_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(results))\n",
        "\n",
        "    return best_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZwrckToK0AN"
      },
      "source": [
        "# Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vYVHMvYIK5VJ"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, tokenizer, eval_dataset=None):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the given dataset and return evaluation metrics.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to be evaluated.\n",
        "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer used\n",
        "            for processing text data.\n",
        "        eval_dataset (TextDataset, optional): The dataset to evaluate the\n",
        "            model on. If None, a default dataset is created from the\n",
        "            evaluation data file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing evaluation metrics and predictions.\n",
        "    \"\"\"\n",
        "    if eval_dataset is None:\n",
        "        eval_dataset = TextDataset(\n",
        "            tokenizer, args.eval_data_file, balance_type=\"full_set_imbalanced\"\n",
        "        )\n",
        "\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(\n",
        "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation *****\")\n",
        "    logger.info(f\"  Num entries = {len(eval_dataset)}\")\n",
        "    logger.info(f\"  Batch size = {args.eval_batch_size}\")\n",
        "\n",
        "    model.eval()\n",
        "    code_vecs, nl_vecs, label_vecs, idx_vecs = [], [], [], []\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        # Get inputs and ground truth labels\n",
        "        code_inputs = batch[\"code_input\"].to(args.device)\n",
        "        nl_inputs = batch[\"nl_input\"].to(args.device)\n",
        "        labels = batch[\"changed\"].to(args.device).float()\n",
        "        idxs = batch[\"idx\"].to(args.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            code_vec = model(code_inputs=code_inputs)\n",
        "            nl_vec = model(nl_inputs=nl_inputs)\n",
        "\n",
        "            code_vecs.append(code_vec)\n",
        "            nl_vecs.append(nl_vec)\n",
        "            label_vecs.append(labels)\n",
        "            idx_vecs.append(idxs)\n",
        "\n",
        "    model.train()\n",
        "    code_vecs = torch.cat(code_vecs, dim=0)\n",
        "    nl_vecs = torch.cat(nl_vecs, dim=0)\n",
        "    label_vecs = torch.cat(label_vecs, dim=0)\n",
        "    idx_vecs = torch.cat(idx_vecs, dim=0)\n",
        "\n",
        "    # Calcuate scores and binary cross entropy/mean squared error\n",
        "    scores = torch.einsum(\"ij,ij->i\", code_vecs, nl_vecs)\n",
        "    mult_scores = scores * 5\n",
        "\n",
        "    predictions = torch.special.expit(scores)\n",
        "    predictions_mult = torch.special.expit(mult_scores)\n",
        "\n",
        "    bce = nn.functional.binary_cross_entropy_with_logits(\n",
        "        scores, label_vecs\n",
        "    )\n",
        "    bce_mult = nn.functional.binary_cross_entropy_with_logits(\n",
        "        mult_scores, label_vecs\n",
        "    )\n",
        "\n",
        "    auprc = binary_auprc(predictions, label_vecs)\n",
        "    auroc = binary_auroc(predictions, label_vecs)\n",
        "\n",
        "    return {\"bce\": float(bce),\n",
        "            \"bce_mult\": float(bce_mult),\n",
        "            \"auprc\": float(auprc),\n",
        "            \"auroc\": float(auroc),\n",
        "            \"predictions\": predictions.tolist(),\n",
        "            \"predictions_mult\": predictions_mult.tolist(),\n",
        "            \"labels\": label_vecs.tolist(),\n",
        "            \"idxs\": idx_vecs.tolist()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J0YDwj_K7xN"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSPdcC8wK9Px",
        "outputId": "1ac37cf3-407a-4d77-c36b-c03f726a53bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "08/28/2024 15:08:09 - INFO - __main__ -   device: cuda, n_gpu: 1\n",
            "<ipython-input-20-69a864b559a9>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_to_load.load_state_dict(torch.load(input_dir, map_location=torch.device('cpu')))\n",
            "08/28/2024 15:08:10 - INFO - __main__ -   Dataset size at start: 463\n",
            "08/28/2024 15:08:11 - INFO - __main__ -     n positive examples = 17\n",
            "08/28/2024 15:08:11 - INFO - __main__ -     n negative examples = 383\n",
            "08/28/2024 15:08:11 - INFO - __main__ -   Final dataset size: 400\n",
            "08/28/2024 15:08:11 - INFO - __main__ -   Removed entries due to exceeding token limit: 63\n",
            "08/28/2024 15:08:11 - INFO - __main__ -   ***** Running evaluation *****\n",
            "08/28/2024 15:08:11 - INFO - __main__ -     Num entries = 400\n",
            "08/28/2024 15:08:11 - INFO - __main__ -     Batch size = 24\n",
            "08/28/2024 15:08:21 - INFO - __main__ -   ***** Eval results *****\n",
            "08/28/2024 15:08:21 - INFO - __main__ -     bce = 0.6933\n",
            "08/28/2024 15:08:21 - INFO - __main__ -     bce_mult = 0.7122\n",
            "08/28/2024 15:08:21 - INFO - __main__ -     auprc = 0.4655\n",
            "08/28/2024 15:08:21 - INFO - __main__ -     auroc = 0.8688\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"device: %s, n_gpu: %s\", args.device, args.n_gpu)\n",
        "\n",
        "#build model\n",
        "tokenizer = RobertaTokenizer.from_pretrained(args.model_name_or_path)\n",
        "model = RobertaModel.from_pretrained(args.model_name_or_path)\n",
        "\n",
        "model = Model(model)\n",
        "model.to(args.device)\n",
        "\n",
        "# Training\n",
        "if args.do_train:\n",
        "    train(model, tokenizer)\n",
        "\n",
        "if args.do_eval:\n",
        "    input_dir = Path(args.output_dir)\n",
        "    input_dir = input_dir.joinpath(\"checkpoint-best/model.bin\")\n",
        "    model_to_load = model.module if hasattr(model, \"module\") else model\n",
        "    model_to_load.load_state_dict(torch.load(\n",
        "        input_dir, map_location=torch.device('cpu'))\n",
        "    )\n",
        "    model.to(args.device)\n",
        "    result = evaluate(model, tokenizer)\n",
        "    logger.info(\"***** Eval results *****\")\n",
        "    for key, value in result.items():\n",
        "        if key not in {\"predictions\", \"predictions_mult\", \"labels\", \"idxs\"}:\n",
        "            logger.info(\"  %s = %s\", key, round(value, 4))\n",
        "\n",
        "    output_dir = Path(args.output_dir)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    results_path = output_dir.joinpath(f\"final_eval.json\")\n",
        "    with results_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeDqB5_t60_o"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}